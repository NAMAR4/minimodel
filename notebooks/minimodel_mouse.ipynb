{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minimodel import data\n",
    "mouse_id = 5\n",
    "\n",
    "data_path = './data'\n",
    "weight_path = './checkpoints_trained'\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw image shape:  (68000, 66, 264)\n",
      "cropped image shape:  (68000, 66, 130)\n",
      "img:  (68000, 66, 130) -2.0829253 2.1060908 float32\n"
     ]
    }
   ],
   "source": [
    "# load images\n",
    "img = data.load_images(data_path, mouse_id, file=data.img_file_name[mouse_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loading activities from ./data/FX20_nat60k_2023_09_29.npz\n"
     ]
    }
   ],
   "source": [
    "# load neurons\n",
    "fname = '%s_nat60k_%s.npz'%(data.db[mouse_id]['mname'], data.db[mouse_id]['datexp'])\n",
    "spks, istim_train, istim_test, xpos, ypos, spks_rep_all = data.load_neurons(file_path = os.path.join(data_path, fname), mouse_id = mouse_id)\n",
    "n_stim, n_neurons = spks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "splitting training and validation set...\n",
      "itrain:  (43081,)\n",
      "ival:  (4787,)\n"
     ]
    }
   ],
   "source": [
    "# split train and validation set\n",
    "itrain, ival = data.split_train_val(istim_train, train_frac=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "normalizing neural data...\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "spks, spks_rep_all = data.normalize_spks(spks, spks_rep_all, itrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ineuron: 12, test_fev: [0.17706311]\n",
      "spks_train:  torch.Size([43081, 1]) tensor(-1.2562e-16, device='cuda:0') tensor(15.5218, device='cuda:0')\n",
      "spks_val:  torch.Size([4787, 1]) tensor(0., device='cuda:0') tensor(13.7926, device='cuda:0')\n",
      "img_train:  torch.Size([43081, 1, 66, 130]) tensor(-2.0829, device='cuda:0') tensor(2.1061, device='cuda:0')\n",
      "img_val:  torch.Size([4787, 1, 66, 130]) tensor(-2.0829, device='cuda:0') tensor(2.1061, device='cuda:0')\n",
      "img_test:  torch.Size([500, 1, 66, 130]) tensor(-2.0829, device='cuda:0') tensor(2.1061, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from minimodel import metrics\n",
    "ineuron = 0\n",
    "test_fev = 0.0\n",
    "for i in range(1000):\n",
    "    ineuron = i\n",
    "    ineur = [ineuron]\n",
    "    spks_rep = []\n",
    "    for i in range(len(spks_rep_all)):\n",
    "        spks_rep.append(spks_rep_all[i][:,ineur])\n",
    "    test_fev = metrics.fev(spks_rep)\n",
    "\n",
    "    if test_fev >= 0.15: break\n",
    "\n",
    "print(f\"ineuron: {ineuron}, test_fev: {test_fev}\")\n",
    "\n",
    "ineur = [ineuron]\n",
    "spks_train = torch.from_numpy(spks[itrain][:,ineur]).to(device)\n",
    "spks_val = torch.from_numpy(spks[ival][:,ineur]).to(device)\n",
    "print('spks_train: ', spks_train.shape, spks_train.min(), spks_train.max())\n",
    "print('spks_val: ', spks_val.shape, spks_val.min(), spks_val.max())\n",
    "\n",
    "img_train = torch.from_numpy(img[istim_train][itrain]).to(device).unsqueeze(1) # change :130 to 25:100 \n",
    "img_val = torch.from_numpy(img[istim_train][ival]).to(device).unsqueeze(1)\n",
    "img_test = torch.from_numpy(img[istim_test]).to(device).unsqueeze(1)\n",
    "\n",
    "print('img_train: ', img_train.shape, img_train.min(), img_train.max())\n",
    "print('img_val: ', img_val.shape, img_val.min(), img_val.max())\n",
    "print('img_test: ', img_test.shape, img_test.min(), img_test.max())\n",
    "\n",
    "input_Ly, input_Lx = img_train.shape[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core shape:  torch.Size([1, 64, 33, 65])\n",
      "input shape of readout:  (64, 33, 65)\n",
      "model name:  FX20_092923_2layer_16_64_clamp_norm_depthsep_pool_nn12_hs3e-02_xrange_176.pt\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "from minimodel import model_builder\n",
    "nlayers = 2\n",
    "nconv1 = 16\n",
    "nconv2 = 64\n",
    "seed = 1\n",
    "hs_readout = 0.03\n",
    "wc_coef = 0.2\n",
    "model, in_channels = model_builder.build_model(NN=1, n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2, Wc_coef=wc_coef)\n",
    "model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], ineuron=ineur[0], n_layers=nlayers, in_channels=in_channels, seed=seed,hs_readout=hs_readout)\n",
    "\n",
    "model_path = os.path.join(weight_path, model_name)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained_model_path:  ./checkpoints_trained/FX20_092923_2layer_16_320_clamp_norm_depthsep_pool_xrange_176.pt\n",
      "Learning rate = 0.001\n",
      "epoch 0, train_loss = 0.6905, val_loss = 0.6213, varexp_val = 0.1552, time 10.06s\n",
      "epoch 1, train_loss = 0.6337, val_loss = 0.6103, varexp_val = 0.1838, time 15.17s\n",
      "epoch 2, train_loss = 0.6298, val_loss = 0.6108, varexp_val = 0.1875, time 20.27s\n",
      "epoch 3, train_loss = 0.6278, val_loss = 0.6143, varexp_val = 0.1875, time 25.37s\n",
      "epoch 4, train_loss = 0.6247, val_loss = 0.6070, varexp_val = 0.1883, time 30.47s\n",
      "epoch 5, train_loss = 0.6230, val_loss = 0.6044, varexp_val = 0.1968, time 35.57s\n",
      "epoch 6, train_loss = 0.6225, val_loss = 0.6037, varexp_val = 0.1958, time 40.67s\n",
      "epoch 7, train_loss = 0.6200, val_loss = 0.6102, varexp_val = 0.1895, time 45.77s\n",
      "epoch 8, train_loss = 0.6199, val_loss = 0.6141, varexp_val = 0.1750, time 50.87s\n",
      "epoch 9, train_loss = 0.6189, val_loss = 0.6054, varexp_val = 0.1947, time 55.98s\n",
      "epoch 10, train_loss = 0.6171, val_loss = 0.6130, varexp_val = 0.1868, time 61.08s\n",
      "Early stopping at epoch 10 due to no improvement in validation varexp.\n",
      "Learning rate = 0.0003333333333333333\n",
      "epoch 0, train_loss = 0.6190, val_loss = 0.6058, varexp_val = 0.1880, time 5.10s\n",
      "Early stopping at epoch 0 due to no improvement in validation varexp.\n",
      "Learning rate = 0.00011111111111111112\n",
      "epoch 0, train_loss = 0.6180, val_loss = 0.6036, varexp_val = 0.1934, time 5.10s\n",
      "Early stopping at epoch 0 due to no improvement in validation varexp.\n",
      "Learning rate = 3.7037037037037037e-05\n",
      "epoch 0, train_loss = 0.6182, val_loss = 0.6030, varexp_val = 0.1959, time 5.10s\n",
      "Early stopping at epoch 0 due to no improvement in validation varexp.\n",
      "saved model ./checkpoints_trained/FX20_092923_2layer_16_64_clamp_norm_depthsep_pool_nn12_hs3e-02_xrange_176.pt\n",
      "loaded model ./checkpoints_trained/FX20_092923_2layer_16_64_clamp_norm_depthsep_pool_nn12_hs3e-02_xrange_176.pt\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "from minimodel import model_trainer\n",
    "if not os.path.exists(model_path):\n",
    "    if mouse_id == 5: pretrained_model_path = os.path.join(weight_path, f'{data.mouse_names[mouse_id]}_{data.exp_date[mouse_id]}_2layer_16_320_clamp_norm_depthsep_pool_xrange_176.pt')\n",
    "    else: pretrained_model_path = os.path.join(weight_path, f'{data.mouse_names[mouse_id]}_{data.exp_date[mouse_id]}_2layer_16_320_clamp_norm_depthsep_pool.pt')\n",
    "    print('pretrained_model_path: ', pretrained_model_path)\n",
    "    pretrained_state_dict = torch.load(pretrained_model_path, map_location=device)\n",
    "    # initialize conv1 with the fullmodel weights\n",
    "    model.core.features.layer0.conv.weight.data = pretrained_state_dict['core.features.layer0.conv.weight']\n",
    "    model.core.features.layer0.conv.weight.requires_grad = False\n",
    "    best_state_dict = model_trainer.train(model, spks_train, spks_val, img_train, img_val, device=device, l2_readout=0.2, hs_readout=hs_readout)\n",
    "    torch.save(best_state_dict, model_path)\n",
    "    print('saved model', model_path)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "print('loaded model', model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_pred:  (500, 1) 0.09825486 2.860871\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "test_pred = model_trainer.test_epoch(model, img_test)\n",
    "print('test_pred: ', test_pred.shape, test_pred.min(), test_pred.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEV (test):  0.17706311\n",
      "FEVE (test):  0.7015559\n"
     ]
    }
   ],
   "source": [
    "from minimodel import metrics\n",
    "spks_rep = []\n",
    "for i in range(len(spks_rep_all)):\n",
    "    spks_rep.append(spks_rep_all[i][:,ineur])\n",
    "test_fev, test_feve = metrics.feve(spks_rep, test_pred)\n",
    "\n",
    "print('FEV (test): ', np.mean(test_fev))\n",
    "print('FEVE (test): ', np.mean(test_feve))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check Wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Wc = model.readout.Wc.detach().cpu().numpy().squeeze()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3,3))\n",
    "ax.plot(np.sort(Wc))\n",
    "ax.set_title('Wc')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check fullmodel performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core shape:  torch.Size([1, 320, 33, 65])\n",
      "input shape of readout:  (320, 33, 65)\n",
      "model name:  FX20_092923_2layer_16_320_clamp_norm_depthsep_pool_xrange_176.pt\n",
      "loaded model ./checkpoints_trained/FX20_092923_2layer_16_320_clamp_norm_depthsep_pool_xrange_176.pt\n"
     ]
    }
   ],
   "source": [
    "from minimodel import model_builder\n",
    "nlayers = 2\n",
    "nconv1 = 16\n",
    "nconv2 = 320\n",
    "fullmodel, in_channels = model_builder.build_model(NN=n_neurons, n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2)\n",
    "model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels)\n",
    "model_path = os.path.join(weight_path, model_name)\n",
    "\n",
    "fullmodel.load_state_dict(torch.load(model_path))\n",
    "print('loaded model', model_path)\n",
    "fullmodel = fullmodel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_pred:  (500, 2746) 0.001676619 9.360834\n"
     ]
    }
   ],
   "source": [
    "test_pred = model_trainer.test_epoch(fullmodel, img_test)\n",
    "print('test_pred: ', test_pred.shape, test_pred.min(), test_pred.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEVE (test):  0.79947907\n"
     ]
    }
   ],
   "source": [
    "from minimodel import metrics\n",
    "test_fev, test_feve = metrics.feve(spks_rep, test_pred[:, ineur])\n",
    "print('FEVE (test): ', np.mean(test_feve))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate category variance (FECV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/text16_FX20_2023_09_29.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load txt16 data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext16_\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.npz\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39m(data\u001b[38;5;241m.\u001b[39mdb[mouse_id][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmname\u001b[39m\u001b[38;5;124m'\u001b[39m], data\u001b[38;5;241m.\u001b[39mdb[mouse_id][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatexp\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m dat \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m txt16_spks_test \u001b[38;5;241m=\u001b[39m dat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mss_all\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m nstim, nrep, nneuron \u001b[38;5;241m=\u001b[39m txt16_spks_test\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/.conda/envs/minimodel-env/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:451\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    449\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    452\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/text16_FX20_2023_09_29.npz'"
     ]
    }
   ],
   "source": [
    "# load txt16 data\n",
    "fname = 'text16_%s_%s.npz'%(data.db[mouse_id]['mname'], data.db[mouse_id]['datexp'])\n",
    "dat = np.load(os.path.join(data_path, fname), allow_pickle=True)\n",
    "txt16_spks_test = dat['ss_all']\n",
    "nstim, nrep, nneuron = txt16_spks_test.shape\n",
    "txt16_istim_test = dat['ss_istim'].astype(int)\n",
    "txt16_istim_test = np.repeat(txt16_istim_test[:, np.newaxis], nrep, axis=1).flatten()\n",
    "txt16_labels_test = dat['ss_labels']\n",
    "txt16_labels_test = np.repeat(txt16_labels_test[:, np.newaxis], nrep, axis=1).flatten()\n",
    "\n",
    "print('txt16_labels_test shape:', txt16_labels_test.shape)\n",
    "\n",
    "txt16_istim_train = dat['istim'].astype(int)\n",
    "txt16_labels_train = dat['labels']\n",
    "\n",
    "print('txt16_labels_train shape:', txt16_labels_train.shape)\n",
    "\n",
    "txt16_labels = np.hstack((txt16_labels_train, txt16_labels_test))\n",
    "txt16_istim = np.hstack((txt16_istim_train, txt16_istim_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load txt16 images\n",
    "img = data.load_images(data_path, mouse_id, file='nat60k_text16.mat', normalize=False)\n",
    "txt16_img = img[txt16_istim]\n",
    "# zscore txt16_imgs\n",
    "img_mean = txt16_img.mean()\n",
    "img_std = txt16_img.std()\n",
    "txt16_img_zscore = (txt16_img - img_mean) / img_std\n",
    "txt16_img_zscore = torch.from_numpy(txt16_img_zscore).to(device).unsqueeze(1)\n",
    "print(txt16_img_zscore.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt16_pred = model_trainer.test_epoch(model, txt16_img_zscore)\n",
    "print('test pred:', txt16_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catvar = metrics.fecv_pairwise(txt16_pred.T, txt16_labels)\n",
    "print(f'FECV (neuron {ineur[0]}): {catvar[0]:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the unique train images\n",
    "Nimgs_unique = img_train.shape[0]\n",
    "\n",
    "# get conv2 features of train images (in batches)\n",
    "model.eval()\n",
    "batch_size = 160\n",
    "nconv2 = 64\n",
    "conv2_fvs = np.zeros((Nimgs_unique, nconv2))\n",
    "for i in range(0, Nimgs_unique, batch_size):\n",
    "    images = img_train[i:i+batch_size].to(device)\n",
    "    conv2_fv = model.core(images)\n",
    "    wxy_fv = torch.einsum('iry, irx, ncyx -> ncr', model.readout.Wy, model.readout.Wx, conv2_fv).detach().cpu().numpy().squeeze()\n",
    "    conv2_fvs[i:i+batch_size] = wxy_fv\n",
    "\n",
    "# sort the features and select top 8 image for each channel\n",
    "fv_isort = np.argsort(-conv2_fvs, axis=0)\n",
    "Wc = model.readout.Wc.detach().cpu().numpy().squeeze()\n",
    "ivalid_Wc = np.where(np.abs(Wc)>0.01)[0]\n",
    "print('ivalid_Wc:', len(ivalid_Wc))\n",
    "fv_isort = fv_isort[:, ivalid_Wc]\n",
    "fv_isort_top8 = fv_isort[:8]\n",
    "Nimg, Nchannel = fv_isort_top8.shape\n",
    "\n",
    "# get mask of the images\n",
    "from minimodel.utils import get_image_mask\n",
    "ineuron_mask_up = get_image_mask(model, Ly=input_Ly, Lx=input_Lx)\n",
    "\n",
    "# get predictions from the training set\n",
    "neuron_activity_model = model_trainer.test_epoch(model, img_train)\n",
    "neuron_activity_model = neuron_activity_model.squeeze()\n",
    "prediction_isort = np.argsort(neuron_activity_model)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from minimodel.utils import add_channel_frame\n",
    "\n",
    "# Parameters for the second plot\n",
    "pad = 5\n",
    "vmin = 0\n",
    "vmax = 255\n",
    "valid_wc = Wc[ivalid_Wc]\n",
    "isort = np.argsort(valid_wc)[::-1]\n",
    "Nchannel = 8\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Arial'\n",
    "# Combined plot layout\n",
    "fig = plt.figure(figsize=(Nimg * 2 + 20, Nchannel * 1.1))\n",
    "gs = plt.GridSpec(Nchannel, Nimg + 4, figure=fig, hspace=0.3, wspace=0.1, width_ratios=[1, 1, 1, 1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n",
    "\n",
    "# Plot one (4x4 grid on the left side, occupying 2 rows per row)\n",
    "nshow = 16\n",
    "for i in range(nshow):\n",
    "    row = (i // 4) * 2\n",
    "    col = i % 4\n",
    "    ax = fig.add_subplot(gs[row:row + 2, col])\n",
    "    ax.imshow(img_train[prediction_isort[i]].cpu().numpy().squeeze() * ineuron_mask_up, cmap='gray', vmin=-1, vmax=1)\n",
    "    ax.axis('off')\n",
    "\n",
    "# Plot two (8xNimg grid on the right side)\n",
    "axs = np.empty((Nchannel, Nimg), dtype=object)\n",
    "for i in range(Nchannel):\n",
    "    if i < 6:\n",
    "        ichannel = i\n",
    "    else:\n",
    "        ichannel = -(Nchannel - i)\n",
    "    for j in range(Nimg):\n",
    "        axs[i, j] = fig.add_subplot(gs[i, j + 4])\n",
    "        # ax = axs[i, j + 4]  # Offset by 4 columns to place it on the right side\n",
    "        axs[i, j].imshow(img_train[fv_isort_top8[j, isort[ichannel]]].cpu().numpy().squeeze() * ineuron_mask_up, cmap='gray', vmin=-1, vmax=1)\n",
    "        axs[i, j].axis('off')\n",
    "    wc_value = valid_wc[isort[ichannel]]\n",
    "    # Determine the frame color and linewidth based on valid_wc[isort[ichannel]]\n",
    "    if wc_value > 0:\n",
    "        color = 'red'\n",
    "    else:\n",
    "        color = 'blue'\n",
    "    add_channel_frame(axs, i, 0, Nimg - 1, color, np.abs(valid_wc[isort[ichannel]]/np.max(np.abs(valid_wc))))\n",
    "\n",
    "    ax = axs[i, Nimg - 1]  # Rightmost axis in the row\n",
    "    if ichannel < 0: ichannel = len(valid_wc) + ichannel\n",
    "    ax.text(1.1, 0.5, f'channel {ichannel+1}', transform=ax.transAxes,\n",
    "            verticalalignment='center', fontsize=16, color='black', alpha=0.8)\n",
    "plt.suptitle(f'neuron {ineur[0]}, FEVE={test_feve[0]:.3f}, FECV={catvar[0]:.3f}', fontsize=18)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimodel-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
