{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b4475f9",
   "metadata": {},
   "source": [
    "### In this notbook I train the mouse fullmodels needed for plotting the figures. I will save the models in a checkpoint folder (checkpoints_trained)\n",
    "\n",
    "I reuse the notbook fullmodel_mouse and fittet it to my purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dcd25ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from minimodel import data\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0852717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_id = 5\n",
    "\n",
    "data_path = './data'\n",
    "weight_path = './checkpoints_trained'\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2f4d6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw image shape:  (68000, 66, 264)\n",
      "cropped image shape:  (68000, 66, 130)\n",
      "img:  (68000, 66, 130) -2.0829253 2.1060908 float32\n"
     ]
    }
   ],
   "source": [
    "# load images\n",
    "img = data.load_images(data_path, mouse_id, file=data.img_file_name[mouse_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26d1ddc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loading activities from ./data/FX20_nat60k_2023_09_29.npz\n"
     ]
    }
   ],
   "source": [
    "# load neurons\n",
    "fname = '%s_nat60k_%s.npz'%(data.db[mouse_id]['mname'], data.db[mouse_id]['datexp'])\n",
    "spks, istim_train, istim_test, xpos, ypos, spks_rep_all = data.load_neurons(file_path = os.path.join(data_path, fname), mouse_id = mouse_id)\n",
    "n_stim, n_neurons = spks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0303ac5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "splitting training and validation set...\n",
      "itrain:  (43081,)\n",
      "ival:  (4787,)\n"
     ]
    }
   ],
   "source": [
    "# split train and validation set\n",
    "itrain, ival = data.split_train_val(istim_train, train_frac=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3fd1bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "normalizing neural data...\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# normalize data\n",
    "spks, spks_rep_all = data.normalize_spks(spks, spks_rep_all, itrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d662583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spks_train:  torch.Size([43081, 2746]) tensor(-1.4092e-15, device='cuda:0') tensor(48.7427, device='cuda:0')\n",
      "spks_val:  torch.Size([4787, 2746]) tensor(-6.8745e-16, device='cuda:0') tensor(44.7361, device='cuda:0')\n",
      "img_train:  torch.Size([43081, 1, 66, 130]) tensor(-2.0829, device='cuda:0') tensor(2.1061, device='cuda:0')\n",
      "img_val:  torch.Size([4787, 1, 66, 130]) tensor(-2.0829, device='cuda:0') tensor(2.1061, device='cuda:0')\n",
      "img_test:  torch.Size([500, 1, 66, 130]) tensor(-2.0829, device='cuda:0') tensor(2.1061, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "ineur = np.arange(0, n_neurons) #np.arange(0, n_neurons, 5)\n",
    "spks_train = torch.from_numpy(spks[itrain][:,ineur]).to(device)\n",
    "spks_val = torch.from_numpy(spks[ival][:,ineur]).to(device)\n",
    "\n",
    "print('spks_train: ', spks_train.shape, spks_train.min(), spks_train.max())\n",
    "print('spks_val: ', spks_val.shape, spks_val.min(), spks_val.max())\n",
    "\n",
    "img_train = torch.from_numpy(img[istim_train][itrain]).to(device).unsqueeze(1) # change :130 to 25:100 \n",
    "img_val = torch.from_numpy(img[istim_train][ival]).to(device).unsqueeze(1)\n",
    "img_test = torch.from_numpy(img[istim_test]).to(device).unsqueeze(1)\n",
    "\n",
    "print('img_train: ', img_train.shape, img_train.min(), img_train.max())\n",
    "print('img_val: ', img_val.shape, img_val.min(), img_val.max())\n",
    "print('img_test: ', img_test.shape, img_test.min(), img_test.max())\n",
    "\n",
    "input_Ly, input_Lx = img_train.shape[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cac3493b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core shape:  torch.Size([1, 192, 33, 65])\n",
      "input shape of readout:  (192, 33, 65)\n",
      "model name:  FX20_092923_1layer_192_clamp_norm_depthsep_pool_xrange_176.pt\n",
      "model path:  ./checkpoints_trained/FX20_092923_1layer_192_clamp_norm_depthsep_pool_xrange_176.pt\n",
      "cuda\n",
      "Learning rate = 0.001\n",
      "epoch 0, train_loss = 0.8544, val_loss = 0.8372, varexp_val = 0.0218, time 36.06s\n",
      "epoch 1, train_loss = 0.8350, val_loss = 0.8255, varexp_val = 0.0382, time 66.54s\n",
      "epoch 2, train_loss = 0.8225, val_loss = 0.8142, varexp_val = 0.0536, time 97.01s\n",
      "epoch 3, train_loss = 0.8147, val_loss = 0.8087, varexp_val = 0.0614, time 127.51s\n",
      "epoch 4, train_loss = 0.8107, val_loss = 0.8057, varexp_val = 0.0657, time 157.98s\n",
      "epoch 5, train_loss = 0.8081, val_loss = 0.8046, varexp_val = 0.0673, time 188.45s\n",
      "epoch 6, train_loss = 0.8061, val_loss = 0.8022, varexp_val = 0.0710, time 218.91s\n",
      "epoch 7, train_loss = 0.8045, val_loss = 0.8010, varexp_val = 0.0728, time 249.38s\n",
      "epoch 8, train_loss = 0.8033, val_loss = 0.7998, varexp_val = 0.0743, time 279.84s\n",
      "epoch 9, train_loss = 0.8021, val_loss = 0.7987, varexp_val = 0.0762, time 310.31s\n",
      "epoch 10, train_loss = 0.8011, val_loss = 0.7981, varexp_val = 0.0771, time 340.77s\n",
      "epoch 11, train_loss = 0.8002, val_loss = 0.7969, varexp_val = 0.0791, time 371.22s\n",
      "epoch 12, train_loss = 0.7995, val_loss = 0.7964, varexp_val = 0.0795, time 401.68s\n",
      "epoch 13, train_loss = 0.7988, val_loss = 0.7971, varexp_val = 0.0791, time 432.14s\n",
      "epoch 14, train_loss = 0.7982, val_loss = 0.7972, varexp_val = 0.0790, time 462.60s\n",
      "epoch 15, train_loss = 0.7975, val_loss = 0.7948, varexp_val = 0.0822, time 493.06s\n",
      "epoch 16, train_loss = 0.7971, val_loss = 0.7942, varexp_val = 0.0830, time 523.52s\n",
      "epoch 17, train_loss = 0.7967, val_loss = 0.7941, varexp_val = 0.0834, time 553.97s\n",
      "epoch 18, train_loss = 0.7963, val_loss = 0.7950, varexp_val = 0.0823, time 584.42s\n",
      "epoch 19, train_loss = 0.7959, val_loss = 0.7937, varexp_val = 0.0843, time 614.88s\n",
      "epoch 20, train_loss = 0.7955, val_loss = 0.7931, varexp_val = 0.0850, time 645.33s\n",
      "epoch 21, train_loss = 0.7951, val_loss = 0.7925, varexp_val = 0.0859, time 675.79s\n",
      "epoch 22, train_loss = 0.7950, val_loss = 0.7931, varexp_val = 0.0851, time 706.25s\n",
      "epoch 23, train_loss = 0.7947, val_loss = 0.7920, varexp_val = 0.0865, time 736.70s\n",
      "epoch 24, train_loss = 0.7943, val_loss = 0.7923, varexp_val = 0.0857, time 767.16s\n",
      "epoch 25, train_loss = 0.7942, val_loss = 0.7916, varexp_val = 0.0872, time 797.62s\n",
      "epoch 26, train_loss = 0.7939, val_loss = 0.7921, varexp_val = 0.0864, time 828.08s\n",
      "epoch 27, train_loss = 0.7937, val_loss = 0.7916, varexp_val = 0.0874, time 858.54s\n",
      "epoch 28, train_loss = 0.7935, val_loss = 0.7918, varexp_val = 0.0870, time 888.99s\n",
      "epoch 29, train_loss = 0.7933, val_loss = 0.7913, varexp_val = 0.0877, time 919.45s\n",
      "epoch 30, train_loss = 0.7933, val_loss = 0.7909, varexp_val = 0.0885, time 949.90s\n",
      "epoch 31, train_loss = 0.7930, val_loss = 0.7909, varexp_val = 0.0883, time 980.35s\n",
      "epoch 32, train_loss = 0.7929, val_loss = 0.7911, varexp_val = 0.0882, time 1010.80s\n",
      "epoch 33, train_loss = 0.7928, val_loss = 0.7908, varexp_val = 0.0886, time 1041.25s\n",
      "epoch 34, train_loss = 0.7925, val_loss = 0.7910, varexp_val = 0.0883, time 1071.70s\n",
      "epoch 35, train_loss = 0.7927, val_loss = 0.7922, varexp_val = 0.0866, time 1102.15s\n",
      "epoch 36, train_loss = 0.7923, val_loss = 0.7902, varexp_val = 0.0894, time 1132.60s\n",
      "epoch 37, train_loss = 0.7922, val_loss = 0.7912, varexp_val = 0.0883, time 1163.05s\n",
      "epoch 38, train_loss = 0.7922, val_loss = 0.7901, varexp_val = 0.0895, time 1193.51s\n",
      "epoch 39, train_loss = 0.7920, val_loss = 0.7902, varexp_val = 0.0897, time 1223.97s\n",
      "epoch 40, train_loss = 0.7919, val_loss = 0.7902, varexp_val = 0.0894, time 1254.45s\n",
      "epoch 41, train_loss = 0.7917, val_loss = 0.7902, varexp_val = 0.0895, time 1284.91s\n",
      "epoch 42, train_loss = 0.7918, val_loss = 0.7900, varexp_val = 0.0895, time 1315.43s\n",
      "epoch 43, train_loss = 0.7916, val_loss = 0.7908, varexp_val = 0.0892, time 1345.96s\n",
      "epoch 44, train_loss = 0.7915, val_loss = 0.7902, varexp_val = 0.0893, time 1376.49s\n",
      "Early stopping at epoch 44 due to no improvement in validation varexp.\n",
      "Learning rate = 0.0003333333333333333\n",
      "epoch 0, train_loss = 0.7899, val_loss = 0.7885, varexp_val = 0.0921, time 30.53s\n",
      "epoch 1, train_loss = 0.7894, val_loss = 0.7884, varexp_val = 0.0923, time 61.09s\n",
      "epoch 2, train_loss = 0.7892, val_loss = 0.7882, varexp_val = 0.0924, time 91.63s\n",
      "epoch 3, train_loss = 0.7892, val_loss = 0.7881, varexp_val = 0.0928, time 122.32s\n",
      "epoch 4, train_loss = 0.7892, val_loss = 0.7881, varexp_val = 0.0929, time 152.80s\n",
      "epoch 5, train_loss = 0.7891, val_loss = 0.7883, varexp_val = 0.0925, time 183.29s\n",
      "epoch 6, train_loss = 0.7890, val_loss = 0.7883, varexp_val = 0.0924, time 213.86s\n",
      "epoch 7, train_loss = 0.7889, val_loss = 0.7879, varexp_val = 0.0932, time 244.35s\n",
      "epoch 8, train_loss = 0.7889, val_loss = 0.7881, varexp_val = 0.0923, time 274.93s\n",
      "epoch 9, train_loss = 0.7888, val_loss = 0.7882, varexp_val = 0.0928, time 305.52s\n",
      "epoch 10, train_loss = 0.7888, val_loss = 0.7880, varexp_val = 0.0929, time 336.07s\n",
      "epoch 11, train_loss = 0.7887, val_loss = 0.7878, varexp_val = 0.0934, time 366.61s\n",
      "epoch 12, train_loss = 0.7887, val_loss = 0.7878, varexp_val = 0.0931, time 397.15s\n",
      "epoch 13, train_loss = 0.7887, val_loss = 0.7885, varexp_val = 0.0922, time 427.70s\n",
      "epoch 14, train_loss = 0.7886, val_loss = 0.7890, varexp_val = 0.0919, time 458.35s\n",
      "epoch 15, train_loss = 0.7885, val_loss = 0.7879, varexp_val = 0.0932, time 488.81s\n",
      "epoch 16, train_loss = 0.7885, val_loss = 0.7878, varexp_val = 0.0932, time 519.30s\n",
      "Early stopping at epoch 16 due to no improvement in validation varexp.\n",
      "Learning rate = 0.00011111111111111112\n",
      "epoch 0, train_loss = 0.7877, val_loss = 0.7873, varexp_val = 0.0941, time 30.54s\n",
      "epoch 1, train_loss = 0.7875, val_loss = 0.7872, varexp_val = 0.0942, time 61.07s\n",
      "epoch 2, train_loss = 0.7874, val_loss = 0.7873, varexp_val = 0.0940, time 91.66s\n",
      "epoch 3, train_loss = 0.7874, val_loss = 0.7871, varexp_val = 0.0945, time 122.25s\n",
      "epoch 4, train_loss = 0.7874, val_loss = 0.7871, varexp_val = 0.0945, time 152.79s\n",
      "epoch 5, train_loss = 0.7874, val_loss = 0.7872, varexp_val = 0.0942, time 183.52s\n",
      "epoch 6, train_loss = 0.7873, val_loss = 0.7874, varexp_val = 0.0938, time 213.99s\n",
      "epoch 7, train_loss = 0.7873, val_loss = 0.7870, varexp_val = 0.0947, time 244.48s\n",
      "epoch 8, train_loss = 0.7873, val_loss = 0.7875, varexp_val = 0.0934, time 274.98s\n",
      "epoch 9, train_loss = 0.7872, val_loss = 0.7872, varexp_val = 0.0942, time 305.57s\n",
      "epoch 10, train_loss = 0.7873, val_loss = 0.7871, varexp_val = 0.0943, time 336.07s\n",
      "epoch 11, train_loss = 0.7872, val_loss = 0.7870, varexp_val = 0.0946, time 366.66s\n",
      "epoch 12, train_loss = 0.7872, val_loss = 0.7870, varexp_val = 0.0944, time 397.26s\n",
      "Early stopping at epoch 12 due to no improvement in validation varexp.\n",
      "Learning rate = 3.7037037037037037e-05\n",
      "epoch 0, train_loss = 0.7869, val_loss = 0.7870, varexp_val = 0.0946, time 30.54s\n",
      "Early stopping at epoch 0 due to no improvement in validation varexp.\n",
      "saved model ./checkpoints_trained/FX20_092923_1layer_192_clamp_norm_depthsep_pool_xrange_176.pt\n",
      "loaded model ./checkpoints_trained/FX20_092923_1layer_192_clamp_norm_depthsep_pool_xrange_176.pt\n",
      "test_pred:  (500, 2746) 0.0029961467 7.2573733\n",
      "FEVE (test, all):  2.0136702\n",
      "filtering neurons with FEV > 0.15\n",
      "valid neurons: 1239 / 2746\n",
      "FEVE (test, FEV>0.15): 0.6027274131774902\n",
      "core shape:  torch.Size([1, 192, 33, 65])\n",
      "input shape of readout:  (192, 33, 65)\n",
      "model name:  FX20_092923_2layer_192_192_clamp_norm_depthsep_pool_xrange_176.pt\n",
      "model path:  ./checkpoints_trained/FX20_092923_2layer_192_192_clamp_norm_depthsep_pool_xrange_176.pt\n",
      "cuda\n",
      "Learning rate = 0.001\n",
      "epoch 0, train_loss = 0.8418, val_loss = 0.8159, varexp_val = 0.0499, time 40.47s\n",
      "epoch 1, train_loss = 0.8109, val_loss = 0.8030, varexp_val = 0.0699, time 80.88s\n",
      "epoch 2, train_loss = 0.8019, val_loss = 0.7955, varexp_val = 0.0814, time 121.31s\n",
      "epoch 3, train_loss = 0.7968, val_loss = 0.7923, varexp_val = 0.0869, time 161.74s\n",
      "epoch 4, train_loss = 0.7941, val_loss = 0.7901, varexp_val = 0.0904, time 202.28s\n",
      "epoch 5, train_loss = 0.7921, val_loss = 0.7897, varexp_val = 0.0913, time 242.81s\n",
      "epoch 6, train_loss = 0.7905, val_loss = 0.7876, varexp_val = 0.0945, time 283.22s\n",
      "epoch 7, train_loss = 0.7894, val_loss = 0.7866, varexp_val = 0.0963, time 323.64s\n",
      "epoch 8, train_loss = 0.7885, val_loss = 0.7863, varexp_val = 0.0964, time 364.05s\n",
      "epoch 9, train_loss = 0.7875, val_loss = 0.7853, varexp_val = 0.0985, time 404.46s\n",
      "epoch 10, train_loss = 0.7869, val_loss = 0.7847, varexp_val = 0.0991, time 444.88s\n",
      "epoch 11, train_loss = 0.7861, val_loss = 0.7847, varexp_val = 0.0996, time 485.29s\n",
      "epoch 12, train_loss = 0.7857, val_loss = 0.7840, varexp_val = 0.1003, time 525.73s\n",
      "epoch 13, train_loss = 0.7852, val_loss = 0.7840, varexp_val = 0.1007, time 566.21s\n",
      "epoch 14, train_loss = 0.7848, val_loss = 0.7853, varexp_val = 0.0997, time 606.62s\n",
      "epoch 15, train_loss = 0.7843, val_loss = 0.7831, varexp_val = 0.1020, time 647.03s\n",
      "epoch 16, train_loss = 0.7840, val_loss = 0.7828, varexp_val = 0.1024, time 687.45s\n",
      "epoch 17, train_loss = 0.7837, val_loss = 0.7833, varexp_val = 0.1023, time 727.86s\n",
      "epoch 18, train_loss = 0.7834, val_loss = 0.7827, varexp_val = 0.1031, time 768.27s\n",
      "epoch 19, train_loss = 0.7832, val_loss = 0.7829, varexp_val = 0.1028, time 808.69s\n",
      "epoch 20, train_loss = 0.7829, val_loss = 0.7826, varexp_val = 0.1028, time 849.11s\n",
      "epoch 21, train_loss = 0.7827, val_loss = 0.7819, varexp_val = 0.1039, time 889.52s\n",
      "epoch 22, train_loss = 0.7826, val_loss = 0.7827, varexp_val = 0.1030, time 929.95s\n",
      "epoch 23, train_loss = 0.7823, val_loss = 0.7820, varexp_val = 0.1037, time 970.37s\n",
      "epoch 24, train_loss = 0.7820, val_loss = 0.7820, varexp_val = 0.1038, time 1010.79s\n",
      "epoch 25, train_loss = 0.7820, val_loss = 0.7817, varexp_val = 0.1042, time 1051.21s\n",
      "epoch 26, train_loss = 0.7818, val_loss = 0.7818, varexp_val = 0.1045, time 1091.63s\n",
      "epoch 27, train_loss = 0.7816, val_loss = 0.7818, varexp_val = 0.1046, time 1132.05s\n",
      "epoch 28, train_loss = 0.7814, val_loss = 0.7812, varexp_val = 0.1052, time 1172.47s\n",
      "epoch 29, train_loss = 0.7814, val_loss = 0.7815, varexp_val = 0.1047, time 1212.89s\n",
      "epoch 30, train_loss = 0.7813, val_loss = 0.7816, varexp_val = 0.1049, time 1253.31s\n",
      "epoch 31, train_loss = 0.7811, val_loss = 0.7815, varexp_val = 0.1050, time 1293.72s\n",
      "epoch 32, train_loss = 0.7810, val_loss = 0.7813, varexp_val = 0.1051, time 1334.15s\n",
      "epoch 33, train_loss = 0.7810, val_loss = 0.7818, varexp_val = 0.1044, time 1374.58s\n",
      "Early stopping at epoch 33 due to no improvement in validation varexp.\n",
      "Learning rate = 0.0003333333333333333\n",
      "epoch 0, train_loss = 0.7785, val_loss = 0.7793, varexp_val = 0.1083, time 40.42s\n",
      "epoch 1, train_loss = 0.7780, val_loss = 0.7795, varexp_val = 0.1082, time 80.85s\n",
      "epoch 2, train_loss = 0.7779, val_loss = 0.7791, varexp_val = 0.1086, time 121.27s\n",
      "epoch 3, train_loss = 0.7778, val_loss = 0.7792, varexp_val = 0.1089, time 161.70s\n",
      "epoch 4, train_loss = 0.7777, val_loss = 0.7792, varexp_val = 0.1086, time 202.11s\n",
      "epoch 5, train_loss = 0.7776, val_loss = 0.7795, varexp_val = 0.1081, time 242.53s\n",
      "epoch 6, train_loss = 0.7775, val_loss = 0.7795, varexp_val = 0.1082, time 282.95s\n",
      "epoch 7, train_loss = 0.7775, val_loss = 0.7792, varexp_val = 0.1088, time 323.36s\n",
      "epoch 8, train_loss = 0.7775, val_loss = 0.7793, varexp_val = 0.1080, time 363.78s\n",
      "Early stopping at epoch 8 due to no improvement in validation varexp.\n",
      "Learning rate = 0.00011111111111111112\n",
      "epoch 0, train_loss = 0.7765, val_loss = 0.7784, varexp_val = 0.1097, time 40.42s\n",
      "epoch 1, train_loss = 0.7762, val_loss = 0.7785, varexp_val = 0.1097, time 80.84s\n",
      "epoch 2, train_loss = 0.7762, val_loss = 0.7784, varexp_val = 0.1097, time 121.26s\n",
      "epoch 3, train_loss = 0.7761, val_loss = 0.7784, varexp_val = 0.1101, time 161.68s\n",
      "epoch 4, train_loss = 0.7761, val_loss = 0.7785, varexp_val = 0.1099, time 202.10s\n",
      "epoch 5, train_loss = 0.7760, val_loss = 0.7784, varexp_val = 0.1098, time 242.55s\n",
      "epoch 6, train_loss = 0.7760, val_loss = 0.7786, varexp_val = 0.1095, time 283.01s\n",
      "epoch 7, train_loss = 0.7760, val_loss = 0.7784, varexp_val = 0.1100, time 323.42s\n",
      "epoch 8, train_loss = 0.7759, val_loss = 0.7784, varexp_val = 0.1095, time 363.83s\n",
      "Early stopping at epoch 8 due to no improvement in validation varexp.\n",
      "Learning rate = 3.7037037037037037e-05\n",
      "epoch 0, train_loss = 0.7756, val_loss = 0.7782, varexp_val = 0.1101, time 40.42s\n",
      "Early stopping at epoch 0 due to no improvement in validation varexp.\n",
      "saved model ./checkpoints_trained/FX20_092923_2layer_192_192_clamp_norm_depthsep_pool_xrange_176.pt\n",
      "loaded model ./checkpoints_trained/FX20_092923_2layer_192_192_clamp_norm_depthsep_pool_xrange_176.pt\n",
      "test_pred:  (500, 2746) 0.001691401 8.5541\n",
      "FEVE (test, all):  2.0446703\n",
      "filtering neurons with FEV > 0.15\n",
      "valid neurons: 1239 / 2746\n",
      "FEVE (test, FEV>0.15): 0.7071778774261475\n",
      "core shape:  torch.Size([1, 192, 33, 65])\n",
      "input shape of readout:  (192, 33, 65)\n",
      "model name:  FX20_092923_3layer_192_192_192_clamp_norm_depthsep_pool_xrange_176.pt\n",
      "model path:  ./checkpoints_trained/FX20_092923_3layer_192_192_192_clamp_norm_depthsep_pool_xrange_176.pt\n",
      "cuda\n",
      "Learning rate = 0.001\n",
      "epoch 0, train_loss = 0.8407, val_loss = 0.8141, varexp_val = 0.0520, time 44.83s\n",
      "epoch 1, train_loss = 0.8098, val_loss = 0.8014, varexp_val = 0.0720, time 89.65s\n",
      "epoch 2, train_loss = 0.7999, val_loss = 0.7932, varexp_val = 0.0845, time 134.48s\n",
      "epoch 3, train_loss = 0.7944, val_loss = 0.7906, varexp_val = 0.0895, time 179.31s\n",
      "epoch 4, train_loss = 0.7916, val_loss = 0.7879, varexp_val = 0.0935, time 224.15s\n",
      "epoch 5, train_loss = 0.7895, val_loss = 0.7871, varexp_val = 0.0952, time 268.99s\n",
      "epoch 6, train_loss = 0.7880, val_loss = 0.7854, varexp_val = 0.0978, time 313.82s\n",
      "epoch 7, train_loss = 0.7869, val_loss = 0.7844, varexp_val = 0.0996, time 358.66s\n",
      "epoch 8, train_loss = 0.7860, val_loss = 0.7841, varexp_val = 0.1000, time 403.49s\n",
      "epoch 9, train_loss = 0.7852, val_loss = 0.7834, varexp_val = 0.1011, time 448.34s\n",
      "epoch 10, train_loss = 0.7846, val_loss = 0.7828, varexp_val = 0.1022, time 493.18s\n",
      "epoch 11, train_loss = 0.7839, val_loss = 0.7831, varexp_val = 0.1026, time 538.02s\n",
      "epoch 12, train_loss = 0.7836, val_loss = 0.7823, varexp_val = 0.1032, time 582.85s\n",
      "epoch 13, train_loss = 0.7831, val_loss = 0.7816, varexp_val = 0.1042, time 627.69s\n",
      "epoch 14, train_loss = 0.7827, val_loss = 0.7837, varexp_val = 0.1023, time 672.53s\n",
      "epoch 15, train_loss = 0.7822, val_loss = 0.7814, varexp_val = 0.1051, time 717.36s\n",
      "epoch 16, train_loss = 0.7820, val_loss = 0.7813, varexp_val = 0.1052, time 762.20s\n",
      "epoch 17, train_loss = 0.7816, val_loss = 0.7813, varexp_val = 0.1057, time 807.03s\n",
      "epoch 18, train_loss = 0.7814, val_loss = 0.7809, varexp_val = 0.1062, time 851.86s\n",
      "epoch 19, train_loss = 0.7811, val_loss = 0.7814, varexp_val = 0.1057, time 896.70s\n",
      "epoch 20, train_loss = 0.7809, val_loss = 0.7806, varexp_val = 0.1062, time 941.53s\n",
      "epoch 21, train_loss = 0.7806, val_loss = 0.7807, varexp_val = 0.1063, time 986.36s\n",
      "epoch 22, train_loss = 0.7805, val_loss = 0.7817, varexp_val = 0.1048, time 1031.20s\n",
      "epoch 23, train_loss = 0.7802, val_loss = 0.7804, varexp_val = 0.1068, time 1076.03s\n",
      "epoch 24, train_loss = 0.7800, val_loss = 0.7803, varexp_val = 0.1068, time 1120.89s\n",
      "epoch 25, train_loss = 0.7799, val_loss = 0.7801, varexp_val = 0.1071, time 1165.72s\n",
      "epoch 26, train_loss = 0.7797, val_loss = 0.7798, varexp_val = 0.1079, time 1210.55s\n",
      "epoch 27, train_loss = 0.7795, val_loss = 0.7794, varexp_val = 0.1083, time 1255.39s\n",
      "epoch 28, train_loss = 0.7793, val_loss = 0.7796, varexp_val = 0.1081, time 1300.22s\n",
      "epoch 29, train_loss = 0.7793, val_loss = 0.7799, varexp_val = 0.1075, time 1345.05s\n",
      "epoch 30, train_loss = 0.7792, val_loss = 0.7798, varexp_val = 0.1078, time 1389.88s\n",
      "epoch 31, train_loss = 0.7790, val_loss = 0.7798, varexp_val = 0.1082, time 1434.71s\n",
      "epoch 32, train_loss = 0.7789, val_loss = 0.7794, varexp_val = 0.1085, time 1479.54s\n",
      "epoch 33, train_loss = 0.7788, val_loss = 0.7797, varexp_val = 0.1078, time 1524.37s\n",
      "epoch 34, train_loss = 0.7787, val_loss = 0.7796, varexp_val = 0.1079, time 1569.20s\n",
      "epoch 35, train_loss = 0.7786, val_loss = 0.7795, varexp_val = 0.1085, time 1614.03s\n",
      "epoch 36, train_loss = 0.7785, val_loss = 0.7793, varexp_val = 0.1086, time 1658.86s\n",
      "epoch 37, train_loss = 0.7784, val_loss = 0.7792, varexp_val = 0.1088, time 1703.70s\n",
      "epoch 38, train_loss = 0.7783, val_loss = 0.7795, varexp_val = 0.1083, time 1748.53s\n",
      "epoch 39, train_loss = 0.7782, val_loss = 0.7794, varexp_val = 0.1086, time 1793.36s\n",
      "epoch 40, train_loss = 0.7781, val_loss = 0.7796, varexp_val = 0.1081, time 1838.18s\n",
      "epoch 41, train_loss = 0.7780, val_loss = 0.7796, varexp_val = 0.1083, time 1883.00s\n",
      "epoch 42, train_loss = 0.7780, val_loss = 0.7792, varexp_val = 0.1088, time 1927.83s\n",
      "Early stopping at epoch 42 due to no improvement in validation varexp.\n",
      "Learning rate = 0.0003333333333333333\n",
      "epoch 0, train_loss = 0.7749, val_loss = 0.7767, varexp_val = 0.1128, time 44.83s\n",
      "epoch 1, train_loss = 0.7743, val_loss = 0.7771, varexp_val = 0.1125, time 89.66s\n",
      "epoch 2, train_loss = 0.7742, val_loss = 0.7767, varexp_val = 0.1130, time 134.49s\n",
      "epoch 3, train_loss = 0.7741, val_loss = 0.7768, varexp_val = 0.1132, time 179.32s\n",
      "epoch 4, train_loss = 0.7741, val_loss = 0.7769, varexp_val = 0.1128, time 224.15s\n",
      "epoch 5, train_loss = 0.7740, val_loss = 0.7770, varexp_val = 0.1126, time 268.98s\n",
      "epoch 6, train_loss = 0.7740, val_loss = 0.7770, varexp_val = 0.1126, time 313.81s\n",
      "epoch 7, train_loss = 0.7739, val_loss = 0.7771, varexp_val = 0.1127, time 358.65s\n",
      "epoch 8, train_loss = 0.7739, val_loss = 0.7770, varexp_val = 0.1123, time 403.48s\n",
      "Early stopping at epoch 8 due to no improvement in validation varexp.\n",
      "Learning rate = 0.00011111111111111112\n",
      "epoch 0, train_loss = 0.7726, val_loss = 0.7759, varexp_val = 0.1144, time 44.83s\n",
      "epoch 1, train_loss = 0.7723, val_loss = 0.7761, varexp_val = 0.1143, time 89.66s\n",
      "epoch 2, train_loss = 0.7723, val_loss = 0.7760, varexp_val = 0.1142, time 134.49s\n",
      "epoch 3, train_loss = 0.7722, val_loss = 0.7759, varexp_val = 0.1144, time 179.32s\n",
      "epoch 4, train_loss = 0.7722, val_loss = 0.7761, varexp_val = 0.1142, time 224.16s\n",
      "epoch 5, train_loss = 0.7721, val_loss = 0.7760, varexp_val = 0.1142, time 268.99s\n",
      "Early stopping at epoch 5 due to no improvement in validation varexp.\n",
      "Learning rate = 3.7037037037037037e-05\n",
      "epoch 0, train_loss = 0.7718, val_loss = 0.7757, varexp_val = 0.1147, time 44.84s\n",
      "epoch 1, train_loss = 0.7717, val_loss = 0.7758, varexp_val = 0.1147, time 89.68s\n",
      "epoch 2, train_loss = 0.7716, val_loss = 0.7758, varexp_val = 0.1146, time 134.52s\n",
      "epoch 3, train_loss = 0.7716, val_loss = 0.7756, varexp_val = 0.1149, time 179.37s\n",
      "epoch 4, train_loss = 0.7716, val_loss = 0.7758, varexp_val = 0.1147, time 224.21s\n",
      "epoch 5, train_loss = 0.7716, val_loss = 0.7757, varexp_val = 0.1146, time 269.04s\n",
      "epoch 6, train_loss = 0.7715, val_loss = 0.7759, varexp_val = 0.1144, time 313.87s\n",
      "epoch 7, train_loss = 0.7715, val_loss = 0.7758, varexp_val = 0.1147, time 358.84s\n",
      "epoch 8, train_loss = 0.7715, val_loss = 0.7757, varexp_val = 0.1144, time 403.67s\n",
      "Early stopping at epoch 8 due to no improvement in validation varexp.\n",
      "saved model ./checkpoints_trained/FX20_092923_3layer_192_192_192_clamp_norm_depthsep_pool_xrange_176.pt\n",
      "loaded model ./checkpoints_trained/FX20_092923_3layer_192_192_192_clamp_norm_depthsep_pool_xrange_176.pt\n",
      "test_pred:  (500, 2746) 0.0052458644 7.505542\n",
      "FEVE (test, all):  2.1320841\n",
      "filtering neurons with FEV > 0.15\n",
      "valid neurons: 1239 / 2746\n",
      "FEVE (test, FEV>0.15): 0.7392747402191162\n",
      "core shape:  torch.Size([1, 192, 33, 65])\n",
      "input shape of readout:  (192, 33, 65)\n",
      "model name:  FX20_092923_4layer_192_192_192_192_clamp_norm_depthsep_pool_xrange_176.pt\n",
      "model path:  ./checkpoints_trained/FX20_092923_4layer_192_192_192_192_clamp_norm_depthsep_pool_xrange_176.pt\n",
      "cuda\n",
      "Learning rate = 0.001\n",
      "epoch 0, train_loss = 0.8418, val_loss = 0.8161, varexp_val = 0.0491, time 49.25s\n",
      "epoch 1, train_loss = 0.8114, val_loss = 0.8025, varexp_val = 0.0699, time 98.48s\n",
      "epoch 2, train_loss = 0.8010, val_loss = 0.7944, varexp_val = 0.0826, time 147.74s\n",
      "epoch 3, train_loss = 0.7950, val_loss = 0.7906, varexp_val = 0.0892, time 196.98s\n",
      "epoch 4, train_loss = 0.7917, val_loss = 0.7877, varexp_val = 0.0937, time 246.22s\n",
      "epoch 5, train_loss = 0.7894, val_loss = 0.7867, varexp_val = 0.0956, time 295.47s\n",
      "epoch 6, train_loss = 0.7878, val_loss = 0.7852, varexp_val = 0.0981, time 344.71s\n",
      "epoch 7, train_loss = 0.7866, val_loss = 0.7845, varexp_val = 0.0994, time 393.95s\n",
      "epoch 8, train_loss = 0.7857, val_loss = 0.7837, varexp_val = 0.1006, time 443.19s\n",
      "epoch 9, train_loss = 0.7847, val_loss = 0.7829, varexp_val = 0.1018, time 492.44s\n",
      "epoch 10, train_loss = 0.7840, val_loss = 0.7822, varexp_val = 0.1033, time 541.68s\n",
      "epoch 11, train_loss = 0.7833, val_loss = 0.7825, varexp_val = 0.1034, time 590.92s\n",
      "epoch 12, train_loss = 0.7829, val_loss = 0.7814, varexp_val = 0.1044, time 640.16s\n",
      "epoch 13, train_loss = 0.7823, val_loss = 0.7810, varexp_val = 0.1052, time 689.40s\n",
      "epoch 14, train_loss = 0.7819, val_loss = 0.7819, varexp_val = 0.1046, time 738.64s\n",
      "epoch 15, train_loss = 0.7814, val_loss = 0.7807, varexp_val = 0.1062, time 787.89s\n",
      "epoch 16, train_loss = 0.7812, val_loss = 0.7803, varexp_val = 0.1067, time 837.14s\n",
      "epoch 17, train_loss = 0.7808, val_loss = 0.7805, varexp_val = 0.1069, time 886.38s\n",
      "epoch 18, train_loss = 0.7805, val_loss = 0.7802, varexp_val = 0.1072, time 935.63s\n",
      "epoch 19, train_loss = 0.7803, val_loss = 0.7809, varexp_val = 0.1066, time 984.86s\n",
      "epoch 20, train_loss = 0.7800, val_loss = 0.7798, varexp_val = 0.1076, time 1034.11s\n",
      "epoch 21, train_loss = 0.7797, val_loss = 0.7801, varexp_val = 0.1073, time 1083.34s\n",
      "epoch 22, train_loss = 0.7796, val_loss = 0.7809, varexp_val = 0.1060, time 1132.57s\n",
      "epoch 23, train_loss = 0.7794, val_loss = 0.7796, varexp_val = 0.1081, time 1181.82s\n",
      "epoch 24, train_loss = 0.7791, val_loss = 0.7793, varexp_val = 0.1084, time 1231.05s\n",
      "epoch 25, train_loss = 0.7790, val_loss = 0.7789, varexp_val = 0.1090, time 1280.32s\n",
      "epoch 26, train_loss = 0.7788, val_loss = 0.7792, varexp_val = 0.1090, time 1329.63s\n",
      "epoch 27, train_loss = 0.7786, val_loss = 0.7789, varexp_val = 0.1091, time 1378.87s\n",
      "epoch 28, train_loss = 0.7785, val_loss = 0.7790, varexp_val = 0.1092, time 1428.11s\n",
      "epoch 29, train_loss = 0.7784, val_loss = 0.7788, varexp_val = 0.1094, time 1477.34s\n",
      "epoch 30, train_loss = 0.7783, val_loss = 0.7793, varexp_val = 0.1086, time 1526.58s\n",
      "epoch 31, train_loss = 0.7782, val_loss = 0.7792, varexp_val = 0.1092, time 1575.82s\n",
      "epoch 32, train_loss = 0.7780, val_loss = 0.7783, varexp_val = 0.1101, time 1625.05s\n",
      "epoch 33, train_loss = 0.7779, val_loss = 0.7789, varexp_val = 0.1092, time 1674.29s\n",
      "epoch 34, train_loss = 0.7778, val_loss = 0.7790, varexp_val = 0.1088, time 1723.53s\n",
      "epoch 35, train_loss = 0.7777, val_loss = 0.7788, varexp_val = 0.1097, time 1772.77s\n",
      "epoch 36, train_loss = 0.7776, val_loss = 0.7786, varexp_val = 0.1097, time 1822.00s\n",
      "epoch 37, train_loss = 0.7775, val_loss = 0.7787, varexp_val = 0.1095, time 1871.24s\n",
      "Early stopping at epoch 37 due to no improvement in validation varexp.\n",
      "Learning rate = 0.0003333333333333333\n",
      "epoch 0, train_loss = 0.7744, val_loss = 0.7760, varexp_val = 0.1139, time 49.25s\n",
      "epoch 1, train_loss = 0.7737, val_loss = 0.7764, varexp_val = 0.1136, time 98.52s\n",
      "epoch 2, train_loss = 0.7736, val_loss = 0.7761, varexp_val = 0.1140, time 147.75s\n",
      "epoch 3, train_loss = 0.7735, val_loss = 0.7762, varexp_val = 0.1141, time 196.99s\n",
      "epoch 4, train_loss = 0.7734, val_loss = 0.7763, varexp_val = 0.1136, time 246.22s\n",
      "epoch 5, train_loss = 0.7733, val_loss = 0.7765, varexp_val = 0.1133, time 295.46s\n",
      "epoch 6, train_loss = 0.7732, val_loss = 0.7764, varexp_val = 0.1136, time 344.70s\n",
      "epoch 7, train_loss = 0.7732, val_loss = 0.7765, varexp_val = 0.1136, time 393.93s\n",
      "epoch 8, train_loss = 0.7731, val_loss = 0.7764, varexp_val = 0.1132, time 443.17s\n",
      "Early stopping at epoch 8 due to no improvement in validation varexp.\n",
      "Learning rate = 0.00011111111111111112\n",
      "epoch 0, train_loss = 0.7718, val_loss = 0.7753, varexp_val = 0.1154, time 49.24s\n",
      "epoch 1, train_loss = 0.7715, val_loss = 0.7755, varexp_val = 0.1151, time 98.49s\n",
      "epoch 2, train_loss = 0.7715, val_loss = 0.7754, varexp_val = 0.1152, time 147.73s\n",
      "epoch 3, train_loss = 0.7714, val_loss = 0.7754, varexp_val = 0.1153, time 196.97s\n",
      "epoch 4, train_loss = 0.7713, val_loss = 0.7755, varexp_val = 0.1152, time 246.20s\n",
      "epoch 5, train_loss = 0.7713, val_loss = 0.7755, varexp_val = 0.1150, time 295.44s\n",
      "Early stopping at epoch 5 due to no improvement in validation varexp.\n",
      "Learning rate = 3.7037037037037037e-05\n",
      "epoch 0, train_loss = 0.7710, val_loss = 0.7751, varexp_val = 0.1157, time 49.24s\n",
      "epoch 1, train_loss = 0.7708, val_loss = 0.7752, varexp_val = 0.1157, time 98.49s\n",
      "epoch 2, train_loss = 0.7708, val_loss = 0.7752, varexp_val = 0.1156, time 147.76s\n",
      "epoch 3, train_loss = 0.7707, val_loss = 0.7751, varexp_val = 0.1159, time 197.15s\n",
      "epoch 4, train_loss = 0.7707, val_loss = 0.7752, varexp_val = 0.1156, time 246.46s\n",
      "epoch 5, train_loss = 0.7707, val_loss = 0.7752, varexp_val = 0.1155, time 295.77s\n",
      "epoch 6, train_loss = 0.7706, val_loss = 0.7754, varexp_val = 0.1152, time 345.01s\n",
      "epoch 7, train_loss = 0.7706, val_loss = 0.7753, varexp_val = 0.1154, time 394.28s\n",
      "epoch 8, train_loss = 0.7706, val_loss = 0.7751, varexp_val = 0.1154, time 443.63s\n",
      "Early stopping at epoch 8 due to no improvement in validation varexp.\n",
      "saved model ./checkpoints_trained/FX20_092923_4layer_192_192_192_192_clamp_norm_depthsep_pool_xrange_176.pt\n",
      "loaded model ./checkpoints_trained/FX20_092923_4layer_192_192_192_192_clamp_norm_depthsep_pool_xrange_176.pt\n",
      "test_pred:  (500, 2746) 0.0070091486 6.9673653\n",
      "FEVE (test, all):  2.0711145\n",
      "filtering neurons with FEV > 0.15\n",
      "valid neurons: 1239 / 2746\n",
      "FEVE (test, FEV>0.15): 0.7455176711082458\n"
     ]
    }
   ],
   "source": [
    "from minimodel import model_builder\n",
    "from minimodel import model_trainer\n",
    "from minimodel import metrics\n",
    "\n",
    "seed = 1\n",
    "feve_nlayers = []\n",
    "for nlayers in range(1, 5):\n",
    "    # Building Model\n",
    "\n",
    "    nconv1 = 192\n",
    "    nconv2 = 192\n",
    "    model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2)\n",
    "    model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels, seed=seed)\n",
    "    \n",
    "    model_path = os.path.join(weight_path, model_name)\n",
    "    print('model path: ', model_path)\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    # Training the model\n",
    "    print(device)\n",
    "    if not os.path.exists(model_path):\n",
    "        best_state_dict = model_trainer.train(model, spks_train, spks_val, img_train, img_val, device=device)\n",
    "        torch.save(best_state_dict, model_path)\n",
    "        print('saved model', model_path)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print('loaded model', model_path)\n",
    "\n",
    "    # test model\n",
    "    test_pred = model_trainer.test_epoch(model, img_test)\n",
    "    print('test_pred: ', test_pred.shape, test_pred.min(), test_pred.max())\n",
    "\n",
    "\n",
    "    test_fev, test_feve = metrics.feve(spks_rep_all, test_pred)\n",
    "    print('FEVE (test, all): ', np.mean(test_feve))\n",
    "\n",
    "    threshold = 0.15\n",
    "    print(f'filtering neurons with FEV > {threshold}')\n",
    "    valid_idxes = np.where(test_fev > threshold)[0]\n",
    "    print(f'valid neurons: {len(valid_idxes)} / {len(test_fev)}')\n",
    "    print(f'FEVE (test, FEV>0.15): {np.mean(test_feve[test_fev > threshold])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7a005b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering neurons with FEV > 0.15\n",
      "valid neurons: 1239 / 2746\n",
      "FEVE (test): 0.7455176711082458\n"
     ]
    }
   ],
   "source": [
    "from minimodel import metrics\n",
    "test_fev, test_feve = metrics.feve(spks_rep_all, test_pred)\n",
    "\n",
    "threshold = 0.15\n",
    "print(f'filtering neurons with FEV > {threshold}')\n",
    "valid_idxes = np.where(test_fev > threshold)[0]\n",
    "print(f'valid neurons: {len(valid_idxes)} / {len(test_fev)}')\n",
    "print(f'FEVE (test): {np.mean(test_feve[test_fev > threshold])}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimodel-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
