{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b4475f9",
   "metadata": {},
   "source": [
    "### In this notbook I train the mouse fullmodels needed for plotting the figures. I will save the models in a checkpoint folder (checkpoints_trained)\n",
    "\n",
    "I reuse the notbook fullmodel_mouse and fittet it to my purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dcd25ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from minimodel import data\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0852717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_id = 4\n",
    "\n",
    "data_path = './data'\n",
    "weight_path = './checkpoints_trained'\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2f4d6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw image shape:  (68000, 66, 264)\n",
      "cropped image shape:  (68000, 66, 130)\n",
      "img:  (68000, 66, 130) -2.062947 2.088608 float32\n"
     ]
    }
   ],
   "source": [
    "# load images\n",
    "img = data.load_images(data_path, mouse_id, file=data.img_file_name[mouse_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26d1ddc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loading activities from ./data/FX8_nat60k_2023_05_16.npz\n"
     ]
    }
   ],
   "source": [
    "# load neurons\n",
    "fname = '%s_nat60k_%s.npz'%(data.db[mouse_id]['mname'], data.db[mouse_id]['datexp'])\n",
    "spks, istim_train, istim_test, xpos, ypos, spks_rep_all = data.load_neurons(file_path = os.path.join(data_path, fname), mouse_id = mouse_id)\n",
    "n_stim, n_neurons = spks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0303ac5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "splitting training and validation set...\n",
      "itrain:  (32885,)\n",
      "ival:  (3654,)\n"
     ]
    }
   ],
   "source": [
    "# split train and validation set\n",
    "itrain, ival = data.split_train_val(istim_train, train_frac=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3fd1bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "normalizing neural data...\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# normalize data\n",
    "spks, spks_rep_all = data.normalize_spks(spks, spks_rep_all, itrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d662583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spks_train:  torch.Size([32885, 5804]) tensor(0., device='cuda:0') tensor(69.0541, device='cuda:0')\n",
      "spks_val:  torch.Size([3654, 5804]) tensor(0., device='cuda:0') tensor(56.4533, device='cuda:0')\n",
      "img_train:  torch.Size([32885, 1, 66, 130]) tensor(-2.0629, device='cuda:0') tensor(2.0886, device='cuda:0')\n",
      "img_val:  torch.Size([3654, 1, 66, 130]) tensor(-2.0629, device='cuda:0') tensor(2.0886, device='cuda:0')\n",
      "img_test:  torch.Size([500, 1, 66, 130]) tensor(-2.0629, device='cuda:0') tensor(2.0886, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "ineur = np.arange(0, n_neurons) #np.arange(0, n_neurons, 5)\n",
    "spks_train = torch.from_numpy(spks[itrain][:,ineur]).to(device)\n",
    "spks_val = torch.from_numpy(spks[ival][:,ineur]).to(device)\n",
    "\n",
    "print('spks_train: ', spks_train.shape, spks_train.min(), spks_train.max())\n",
    "print('spks_val: ', spks_val.shape, spks_val.min(), spks_val.max())\n",
    "\n",
    "img_train = torch.from_numpy(img[istim_train][itrain]).to(device).unsqueeze(1) # change :130 to 25:100 \n",
    "img_val = torch.from_numpy(img[istim_train][ival]).to(device).unsqueeze(1)\n",
    "img_test = torch.from_numpy(img[istim_test]).to(device).unsqueeze(1)\n",
    "\n",
    "print('img_train: ', img_train.shape, img_train.min(), img_train.max())\n",
    "print('img_val: ', img_val.shape, img_val.min(), img_val.max())\n",
    "print('img_test: ', img_test.shape, img_test.min(), img_test.max())\n",
    "\n",
    "input_Ly, input_Lx = img_train.shape[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cac3493b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core shape:  torch.Size([1, 192, 33, 65])\n",
      "input shape of readout:  (192, 33, 65)\n",
      "model name:  FX8_051623_1layer_192_clamp_norm_depthsep_pool.pt\n",
      "model path:  ./checkpoints_trained/FX8_051623_1layer_192_clamp_norm_depthsep_pool.pt\n",
      "cuda\n",
      "Learning rate = 0.001\n",
      "epoch 0, train_loss = 0.8533, val_loss = 0.8408, varexp_val = 0.0102, time 41.82s\n",
      "epoch 1, train_loss = 0.8374, val_loss = 0.8358, varexp_val = 0.0166, time 79.99s\n",
      "epoch 2, train_loss = 0.8322, val_loss = 0.8309, varexp_val = 0.0235, time 118.18s\n",
      "epoch 3, train_loss = 0.8265, val_loss = 0.8252, varexp_val = 0.0310, time 156.33s\n",
      "epoch 4, train_loss = 0.8206, val_loss = 0.8208, varexp_val = 0.0369, time 194.50s\n",
      "epoch 5, train_loss = 0.8168, val_loss = 0.8160, varexp_val = 0.0431, time 232.67s\n",
      "epoch 6, train_loss = 0.8144, val_loss = 0.8150, varexp_val = 0.0444, time 270.83s\n",
      "epoch 7, train_loss = 0.8127, val_loss = 0.8150, varexp_val = 0.0448, time 308.98s\n",
      "epoch 8, train_loss = 0.8113, val_loss = 0.8121, varexp_val = 0.0486, time 347.15s\n",
      "epoch 9, train_loss = 0.8101, val_loss = 0.8108, varexp_val = 0.0503, time 385.31s\n",
      "epoch 10, train_loss = 0.8090, val_loss = 0.8100, varexp_val = 0.0517, time 423.48s\n",
      "epoch 11, train_loss = 0.8081, val_loss = 0.8100, varexp_val = 0.0519, time 461.63s\n",
      "epoch 12, train_loss = 0.8074, val_loss = 0.8095, varexp_val = 0.0528, time 499.80s\n",
      "epoch 13, train_loss = 0.8067, val_loss = 0.8076, varexp_val = 0.0551, time 537.96s\n",
      "epoch 14, train_loss = 0.8058, val_loss = 0.8079, varexp_val = 0.0550, time 576.10s\n",
      "epoch 15, train_loss = 0.8053, val_loss = 0.8077, varexp_val = 0.0552, time 614.26s\n",
      "epoch 16, train_loss = 0.8047, val_loss = 0.8061, varexp_val = 0.0572, time 652.42s\n",
      "epoch 17, train_loss = 0.8043, val_loss = 0.8059, varexp_val = 0.0577, time 690.58s\n",
      "epoch 18, train_loss = 0.8037, val_loss = 0.8055, varexp_val = 0.0583, time 728.74s\n",
      "epoch 19, train_loss = 0.8032, val_loss = 0.8053, varexp_val = 0.0584, time 766.89s\n",
      "epoch 20, train_loss = 0.8028, val_loss = 0.8042, varexp_val = 0.0601, time 805.03s\n",
      "epoch 21, train_loss = 0.8023, val_loss = 0.8045, varexp_val = 0.0597, time 843.19s\n",
      "epoch 22, train_loss = 0.8019, val_loss = 0.8040, varexp_val = 0.0603, time 881.34s\n",
      "epoch 23, train_loss = 0.8016, val_loss = 0.8035, varexp_val = 0.0611, time 919.49s\n",
      "epoch 24, train_loss = 0.8012, val_loss = 0.8036, varexp_val = 0.0606, time 957.64s\n",
      "epoch 25, train_loss = 0.8010, val_loss = 0.8031, varexp_val = 0.0617, time 995.79s\n",
      "epoch 26, train_loss = 0.8006, val_loss = 0.8029, varexp_val = 0.0618, time 1033.93s\n",
      "epoch 27, train_loss = 0.8003, val_loss = 0.8027, varexp_val = 0.0624, time 1072.07s\n",
      "epoch 28, train_loss = 0.8000, val_loss = 0.8032, varexp_val = 0.0616, time 1110.21s\n",
      "epoch 29, train_loss = 0.7997, val_loss = 0.8020, varexp_val = 0.0635, time 1148.36s\n",
      "epoch 30, train_loss = 0.7994, val_loss = 0.8020, varexp_val = 0.0636, time 1186.44s\n",
      "epoch 31, train_loss = 0.7993, val_loss = 0.8019, varexp_val = 0.0637, time 1224.58s\n",
      "epoch 32, train_loss = 0.7990, val_loss = 0.8024, varexp_val = 0.0629, time 1262.67s\n",
      "epoch 33, train_loss = 0.7987, val_loss = 0.8013, varexp_val = 0.0644, time 1300.74s\n",
      "epoch 34, train_loss = 0.7986, val_loss = 0.8012, varexp_val = 0.0647, time 1338.80s\n",
      "epoch 35, train_loss = 0.7982, val_loss = 0.8013, varexp_val = 0.0642, time 1376.87s\n",
      "epoch 36, train_loss = 0.7981, val_loss = 0.8010, varexp_val = 0.0651, time 1414.94s\n",
      "epoch 37, train_loss = 0.7980, val_loss = 0.8005, varexp_val = 0.0656, time 1453.00s\n",
      "epoch 38, train_loss = 0.7977, val_loss = 0.8004, varexp_val = 0.0659, time 1491.10s\n",
      "epoch 39, train_loss = 0.7977, val_loss = 0.8005, varexp_val = 0.0655, time 1529.18s\n",
      "epoch 40, train_loss = 0.7975, val_loss = 0.8001, varexp_val = 0.0663, time 1567.31s\n",
      "epoch 41, train_loss = 0.7973, val_loss = 0.8002, varexp_val = 0.0664, time 1605.42s\n",
      "epoch 42, train_loss = 0.7972, val_loss = 0.7999, varexp_val = 0.0664, time 1643.48s\n",
      "epoch 43, train_loss = 0.7970, val_loss = 0.8003, varexp_val = 0.0663, time 1681.54s\n",
      "epoch 44, train_loss = 0.7969, val_loss = 0.8001, varexp_val = 0.0663, time 1719.61s\n",
      "epoch 45, train_loss = 0.7967, val_loss = 0.7996, varexp_val = 0.0669, time 1757.67s\n",
      "epoch 46, train_loss = 0.7967, val_loss = 0.8000, varexp_val = 0.0662, time 1795.74s\n",
      "epoch 47, train_loss = 0.7966, val_loss = 0.7995, varexp_val = 0.0670, time 1833.80s\n",
      "epoch 48, train_loss = 0.7964, val_loss = 0.7997, varexp_val = 0.0666, time 1871.87s\n",
      "epoch 49, train_loss = 0.7965, val_loss = 0.7995, varexp_val = 0.0674, time 1909.93s\n",
      "epoch 50, train_loss = 0.7963, val_loss = 0.7994, varexp_val = 0.0669, time 1947.99s\n",
      "epoch 51, train_loss = 0.7962, val_loss = 0.8006, varexp_val = 0.0659, time 1986.04s\n",
      "epoch 52, train_loss = 0.7961, val_loss = 0.7990, varexp_val = 0.0677, time 2024.13s\n",
      "epoch 53, train_loss = 0.7961, val_loss = 0.7996, varexp_val = 0.0671, time 2062.26s\n",
      "epoch 54, train_loss = 0.7959, val_loss = 0.7989, varexp_val = 0.0682, time 2100.32s\n",
      "epoch 55, train_loss = 0.7960, val_loss = 0.7990, varexp_val = 0.0681, time 2138.41s\n",
      "epoch 56, train_loss = 0.7958, val_loss = 0.7993, varexp_val = 0.0676, time 2176.48s\n",
      "epoch 57, train_loss = 0.7957, val_loss = 0.7992, varexp_val = 0.0674, time 2214.55s\n",
      "epoch 58, train_loss = 0.7957, val_loss = 0.8000, varexp_val = 0.0665, time 2252.62s\n",
      "epoch 59, train_loss = 0.7956, val_loss = 0.7990, varexp_val = 0.0679, time 2290.68s\n",
      "Early stopping at epoch 59 due to no improvement in validation varexp.\n",
      "Learning rate = 0.0003333333333333333\n",
      "epoch 0, train_loss = 0.7938, val_loss = 0.7974, varexp_val = 0.0702, time 38.07s\n",
      "epoch 1, train_loss = 0.7933, val_loss = 0.7975, varexp_val = 0.0701, time 76.13s\n",
      "epoch 2, train_loss = 0.7932, val_loss = 0.7976, varexp_val = 0.0702, time 114.19s\n",
      "epoch 3, train_loss = 0.7931, val_loss = 0.7972, varexp_val = 0.0708, time 152.26s\n",
      "epoch 4, train_loss = 0.7930, val_loss = 0.7972, varexp_val = 0.0706, time 190.33s\n",
      "epoch 5, train_loss = 0.7930, val_loss = 0.7973, varexp_val = 0.0708, time 228.40s\n",
      "epoch 6, train_loss = 0.7929, val_loss = 0.7972, varexp_val = 0.0706, time 266.45s\n",
      "epoch 7, train_loss = 0.7929, val_loss = 0.7973, varexp_val = 0.0706, time 304.52s\n",
      "epoch 8, train_loss = 0.7927, val_loss = 0.7973, varexp_val = 0.0704, time 342.58s\n",
      "epoch 9, train_loss = 0.7927, val_loss = 0.7971, varexp_val = 0.0708, time 380.64s\n",
      "epoch 10, train_loss = 0.7927, val_loss = 0.7976, varexp_val = 0.0704, time 418.70s\n",
      "Early stopping at epoch 10 due to no improvement in validation varexp.\n",
      "Learning rate = 0.00011111111111111112\n",
      "epoch 0, train_loss = 0.7920, val_loss = 0.7967, varexp_val = 0.0714, time 38.05s\n",
      "epoch 1, train_loss = 0.7918, val_loss = 0.7967, varexp_val = 0.0713, time 76.11s\n",
      "epoch 2, train_loss = 0.7917, val_loss = 0.7967, varexp_val = 0.0714, time 114.17s\n",
      "epoch 3, train_loss = 0.7917, val_loss = 0.7965, varexp_val = 0.0717, time 152.23s\n",
      "epoch 4, train_loss = 0.7916, val_loss = 0.7966, varexp_val = 0.0716, time 190.30s\n",
      "epoch 5, train_loss = 0.7916, val_loss = 0.7968, varexp_val = 0.0716, time 228.36s\n",
      "epoch 6, train_loss = 0.7916, val_loss = 0.7966, varexp_val = 0.0716, time 266.42s\n",
      "epoch 7, train_loss = 0.7915, val_loss = 0.7966, varexp_val = 0.0717, time 304.48s\n",
      "epoch 8, train_loss = 0.7915, val_loss = 0.7967, varexp_val = 0.0713, time 342.54s\n",
      "Early stopping at epoch 8 due to no improvement in validation varexp.\n",
      "Learning rate = 3.7037037037037037e-05\n",
      "epoch 0, train_loss = 0.7913, val_loss = 0.7965, varexp_val = 0.0718, time 38.07s\n",
      "epoch 1, train_loss = 0.7912, val_loss = 0.7965, varexp_val = 0.0716, time 76.13s\n",
      "epoch 2, train_loss = 0.7912, val_loss = 0.7965, varexp_val = 0.0717, time 114.20s\n",
      "epoch 3, train_loss = 0.7912, val_loss = 0.7964, varexp_val = 0.0720, time 152.26s\n",
      "epoch 4, train_loss = 0.7911, val_loss = 0.7964, varexp_val = 0.0718, time 190.32s\n",
      "epoch 5, train_loss = 0.7911, val_loss = 0.7967, varexp_val = 0.0718, time 228.38s\n",
      "epoch 6, train_loss = 0.7911, val_loss = 0.7965, varexp_val = 0.0717, time 266.44s\n",
      "epoch 7, train_loss = 0.7911, val_loss = 0.7965, varexp_val = 0.0718, time 304.50s\n",
      "epoch 8, train_loss = 0.7911, val_loss = 0.7966, varexp_val = 0.0715, time 342.56s\n",
      "Early stopping at epoch 8 due to no improvement in validation varexp.\n",
      "saved model ./checkpoints_trained/FX8_051623_1layer_192_clamp_norm_depthsep_pool.pt\n",
      "loaded model ./checkpoints_trained/FX8_051623_1layer_192_clamp_norm_depthsep_pool.pt\n",
      "test_pred:  (500, 5804) 0.0020308495 5.615276\n",
      "FEVE (test, all):  0.4730299\n",
      "filtering neurons with FEV > 0.15\n",
      "valid neurons: 2217 / 5804\n",
      "FEVE (test, FEV>0.15): 0.5543037056922913\n",
      "core shape:  torch.Size([1, 192, 33, 65])\n",
      "input shape of readout:  (192, 33, 65)\n",
      "model name:  FX8_051623_2layer_192_192_clamp_norm_depthsep_pool.pt\n",
      "model path:  ./checkpoints_trained/FX8_051623_2layer_192_192_clamp_norm_depthsep_pool.pt\n",
      "cuda\n",
      "Learning rate = 0.001\n",
      "epoch 0, train_loss = 0.8460, val_loss = 0.8260, varexp_val = 0.0285, time 45.72s\n",
      "epoch 1, train_loss = 0.8193, val_loss = 0.8163, varexp_val = 0.0418, time 91.38s\n",
      "epoch 2, train_loss = 0.8121, val_loss = 0.8113, varexp_val = 0.0498, time 137.05s\n",
      "epoch 3, train_loss = 0.8072, val_loss = 0.8060, varexp_val = 0.0577, time 182.71s\n",
      "epoch 4, train_loss = 0.8032, val_loss = 0.8037, varexp_val = 0.0613, time 228.38s\n",
      "epoch 5, train_loss = 0.8005, val_loss = 0.8018, varexp_val = 0.0643, time 274.04s\n",
      "epoch 6, train_loss = 0.7986, val_loss = 0.7998, varexp_val = 0.0676, time 319.70s\n",
      "epoch 7, train_loss = 0.7969, val_loss = 0.7987, varexp_val = 0.0694, time 365.37s\n",
      "epoch 8, train_loss = 0.7955, val_loss = 0.7973, varexp_val = 0.0714, time 411.03s\n",
      "epoch 9, train_loss = 0.7943, val_loss = 0.7963, varexp_val = 0.0731, time 456.70s\n",
      "epoch 10, train_loss = 0.7933, val_loss = 0.7960, varexp_val = 0.0738, time 502.37s\n",
      "epoch 11, train_loss = 0.7923, val_loss = 0.7958, varexp_val = 0.0742, time 548.03s\n",
      "epoch 12, train_loss = 0.7917, val_loss = 0.7957, varexp_val = 0.0747, time 593.69s\n",
      "epoch 13, train_loss = 0.7911, val_loss = 0.7939, varexp_val = 0.0769, time 639.34s\n",
      "epoch 14, train_loss = 0.7903, val_loss = 0.7939, varexp_val = 0.0772, time 684.98s\n",
      "epoch 15, train_loss = 0.7898, val_loss = 0.7936, varexp_val = 0.0776, time 730.69s\n",
      "epoch 16, train_loss = 0.7893, val_loss = 0.7931, varexp_val = 0.0782, time 776.34s\n",
      "epoch 17, train_loss = 0.7889, val_loss = 0.7932, varexp_val = 0.0784, time 821.93s\n",
      "epoch 18, train_loss = 0.7885, val_loss = 0.7920, varexp_val = 0.0801, time 867.53s\n",
      "epoch 19, train_loss = 0.7882, val_loss = 0.7924, varexp_val = 0.0796, time 913.12s\n",
      "epoch 20, train_loss = 0.7877, val_loss = 0.7918, varexp_val = 0.0806, time 958.70s\n",
      "epoch 21, train_loss = 0.7873, val_loss = 0.7918, varexp_val = 0.0803, time 1004.30s\n",
      "epoch 22, train_loss = 0.7870, val_loss = 0.7919, varexp_val = 0.0803, time 1049.90s\n",
      "epoch 23, train_loss = 0.7870, val_loss = 0.7915, varexp_val = 0.0815, time 1095.49s\n",
      "epoch 24, train_loss = 0.7866, val_loss = 0.7912, varexp_val = 0.0812, time 1141.09s\n",
      "epoch 25, train_loss = 0.7864, val_loss = 0.7907, varexp_val = 0.0822, time 1186.69s\n",
      "epoch 26, train_loss = 0.7861, val_loss = 0.7913, varexp_val = 0.0810, time 1232.29s\n",
      "epoch 27, train_loss = 0.7858, val_loss = 0.7908, varexp_val = 0.0821, time 1277.88s\n",
      "epoch 28, train_loss = 0.7857, val_loss = 0.7903, varexp_val = 0.0826, time 1323.48s\n",
      "epoch 29, train_loss = 0.7855, val_loss = 0.7906, varexp_val = 0.0825, time 1369.07s\n",
      "epoch 30, train_loss = 0.7854, val_loss = 0.7913, varexp_val = 0.0820, time 1414.67s\n",
      "epoch 31, train_loss = 0.7852, val_loss = 0.7912, varexp_val = 0.0818, time 1460.26s\n",
      "epoch 32, train_loss = 0.7850, val_loss = 0.7905, varexp_val = 0.0824, time 1505.87s\n",
      "epoch 33, train_loss = 0.7848, val_loss = 0.7904, varexp_val = 0.0828, time 1551.54s\n",
      "epoch 34, train_loss = 0.7847, val_loss = 0.7906, varexp_val = 0.0827, time 1597.22s\n",
      "epoch 35, train_loss = 0.7845, val_loss = 0.7905, varexp_val = 0.0823, time 1642.81s\n",
      "epoch 36, train_loss = 0.7843, val_loss = 0.7901, varexp_val = 0.0833, time 1688.41s\n",
      "epoch 37, train_loss = 0.7843, val_loss = 0.7897, varexp_val = 0.0837, time 1733.99s\n",
      "epoch 38, train_loss = 0.7843, val_loss = 0.7903, varexp_val = 0.0829, time 1779.58s\n",
      "epoch 39, train_loss = 0.7840, val_loss = 0.7902, varexp_val = 0.0831, time 1825.17s\n",
      "epoch 40, train_loss = 0.7840, val_loss = 0.7893, varexp_val = 0.0844, time 1870.80s\n",
      "epoch 41, train_loss = 0.7838, val_loss = 0.7901, varexp_val = 0.0836, time 1916.44s\n",
      "epoch 42, train_loss = 0.7838, val_loss = 0.7903, varexp_val = 0.0830, time 1962.08s\n",
      "epoch 43, train_loss = 0.7836, val_loss = 0.7909, varexp_val = 0.0828, time 2007.69s\n",
      "epoch 44, train_loss = 0.7835, val_loss = 0.7901, varexp_val = 0.0834, time 2053.29s\n",
      "epoch 45, train_loss = 0.7834, val_loss = 0.7899, varexp_val = 0.0837, time 2098.89s\n",
      "Early stopping at epoch 45 due to no improvement in validation varexp.\n",
      "Learning rate = 0.0003333333333333333\n",
      "epoch 0, train_loss = 0.7808, val_loss = 0.7878, varexp_val = 0.0866, time 45.60s\n",
      "epoch 1, train_loss = 0.7802, val_loss = 0.7879, varexp_val = 0.0866, time 91.20s\n",
      "epoch 2, train_loss = 0.7800, val_loss = 0.7877, varexp_val = 0.0874, time 136.80s\n",
      "epoch 3, train_loss = 0.7800, val_loss = 0.7877, varexp_val = 0.0872, time 182.40s\n",
      "epoch 4, train_loss = 0.7799, val_loss = 0.7875, varexp_val = 0.0874, time 228.00s\n",
      "epoch 5, train_loss = 0.7798, val_loss = 0.7881, varexp_val = 0.0867, time 273.60s\n",
      "epoch 6, train_loss = 0.7798, val_loss = 0.7881, varexp_val = 0.0870, time 319.24s\n",
      "epoch 7, train_loss = 0.7797, val_loss = 0.7878, varexp_val = 0.0871, time 364.88s\n",
      "epoch 8, train_loss = 0.7796, val_loss = 0.7880, varexp_val = 0.0867, time 410.50s\n",
      "epoch 9, train_loss = 0.7796, val_loss = 0.7876, varexp_val = 0.0874, time 456.10s\n",
      "Early stopping at epoch 9 due to no improvement in validation varexp.\n",
      "Learning rate = 0.00011111111111111112\n",
      "epoch 0, train_loss = 0.7785, val_loss = 0.7870, varexp_val = 0.0880, time 45.59s\n",
      "epoch 1, train_loss = 0.7783, val_loss = 0.7871, varexp_val = 0.0880, time 91.18s\n",
      "epoch 2, train_loss = 0.7781, val_loss = 0.7873, varexp_val = 0.0882, time 136.78s\n",
      "epoch 3, train_loss = 0.7781, val_loss = 0.7871, varexp_val = 0.0884, time 182.38s\n",
      "epoch 4, train_loss = 0.7781, val_loss = 0.7871, varexp_val = 0.0882, time 227.97s\n",
      "epoch 5, train_loss = 0.7780, val_loss = 0.7874, varexp_val = 0.0880, time 273.60s\n",
      "epoch 6, train_loss = 0.7780, val_loss = 0.7875, varexp_val = 0.0880, time 319.19s\n",
      "epoch 7, train_loss = 0.7780, val_loss = 0.7871, varexp_val = 0.0883, time 364.79s\n",
      "epoch 8, train_loss = 0.7779, val_loss = 0.7872, varexp_val = 0.0879, time 410.39s\n",
      "Early stopping at epoch 8 due to no improvement in validation varexp.\n",
      "Learning rate = 3.7037037037037037e-05\n",
      "epoch 0, train_loss = 0.7775, val_loss = 0.7869, varexp_val = 0.0883, time 45.60s\n",
      "Early stopping at epoch 0 due to no improvement in validation varexp.\n",
      "saved model ./checkpoints_trained/FX8_051623_2layer_192_192_clamp_norm_depthsep_pool.pt\n",
      "loaded model ./checkpoints_trained/FX8_051623_2layer_192_192_clamp_norm_depthsep_pool.pt\n",
      "test_pred:  (500, 5804) 0.0092225075 6.9468794\n",
      "FEVE (test, all):  0.59459925\n",
      "filtering neurons with FEV > 0.15\n",
      "valid neurons: 2217 / 5804\n",
      "FEVE (test, FEV>0.15): 0.6746330261230469\n",
      "core shape:  torch.Size([1, 192, 33, 65])\n",
      "input shape of readout:  (192, 33, 65)\n",
      "model name:  FX8_051623_3layer_192_192_192_clamp_norm_depthsep_pool.pt\n",
      "model path:  ./checkpoints_trained/FX8_051623_3layer_192_192_192_clamp_norm_depthsep_pool.pt\n",
      "cuda\n",
      "Learning rate = 0.001\n",
      "epoch 0, train_loss = 0.8475, val_loss = 0.8275, varexp_val = 0.0265, time 48.96s\n",
      "epoch 1, train_loss = 0.8195, val_loss = 0.8162, varexp_val = 0.0423, time 97.94s\n",
      "epoch 2, train_loss = 0.8116, val_loss = 0.8105, varexp_val = 0.0513, time 146.91s\n",
      "epoch 3, train_loss = 0.8062, val_loss = 0.8053, varexp_val = 0.0585, time 195.88s\n",
      "epoch 4, train_loss = 0.8016, val_loss = 0.8013, varexp_val = 0.0644, time 244.85s\n",
      "epoch 5, train_loss = 0.7983, val_loss = 0.7992, varexp_val = 0.0678, time 293.83s\n",
      "epoch 6, train_loss = 0.7959, val_loss = 0.7971, varexp_val = 0.0715, time 342.80s\n",
      "epoch 7, train_loss = 0.7941, val_loss = 0.7956, varexp_val = 0.0739, time 391.78s\n",
      "epoch 8, train_loss = 0.7928, val_loss = 0.7947, varexp_val = 0.0752, time 440.76s\n",
      "epoch 9, train_loss = 0.7915, val_loss = 0.7938, varexp_val = 0.0772, time 489.73s\n",
      "epoch 10, train_loss = 0.7905, val_loss = 0.7936, varexp_val = 0.0775, time 538.71s\n",
      "epoch 11, train_loss = 0.7896, val_loss = 0.7933, varexp_val = 0.0780, time 587.69s\n",
      "epoch 12, train_loss = 0.7889, val_loss = 0.7930, varexp_val = 0.0790, time 636.66s\n",
      "epoch 13, train_loss = 0.7884, val_loss = 0.7914, varexp_val = 0.0809, time 685.63s\n",
      "epoch 14, train_loss = 0.7875, val_loss = 0.7917, varexp_val = 0.0807, time 734.61s\n",
      "epoch 15, train_loss = 0.7870, val_loss = 0.7912, varexp_val = 0.0815, time 783.57s\n",
      "epoch 16, train_loss = 0.7866, val_loss = 0.7904, varexp_val = 0.0826, time 832.55s\n",
      "epoch 17, train_loss = 0.7861, val_loss = 0.7904, varexp_val = 0.0832, time 881.52s\n",
      "epoch 18, train_loss = 0.7858, val_loss = 0.7899, varexp_val = 0.0838, time 930.50s\n",
      "epoch 19, train_loss = 0.7854, val_loss = 0.7896, varexp_val = 0.0840, time 979.47s\n",
      "epoch 20, train_loss = 0.7850, val_loss = 0.7905, varexp_val = 0.0832, time 1028.45s\n",
      "epoch 21, train_loss = 0.7847, val_loss = 0.7896, varexp_val = 0.0841, time 1077.43s\n",
      "epoch 22, train_loss = 0.7844, val_loss = 0.7894, varexp_val = 0.0845, time 1126.42s\n",
      "epoch 23, train_loss = 0.7842, val_loss = 0.7895, varexp_val = 0.0849, time 1175.39s\n",
      "epoch 24, train_loss = 0.7839, val_loss = 0.7892, varexp_val = 0.0846, time 1224.37s\n",
      "epoch 25, train_loss = 0.7837, val_loss = 0.7884, varexp_val = 0.0863, time 1273.33s\n",
      "epoch 26, train_loss = 0.7833, val_loss = 0.7891, varexp_val = 0.0848, time 1322.30s\n",
      "epoch 27, train_loss = 0.7831, val_loss = 0.7884, varexp_val = 0.0862, time 1371.27s\n",
      "epoch 28, train_loss = 0.7830, val_loss = 0.7889, varexp_val = 0.0853, time 1420.24s\n",
      "epoch 29, train_loss = 0.7827, val_loss = 0.7888, varexp_val = 0.0860, time 1469.20s\n",
      "epoch 30, train_loss = 0.7825, val_loss = 0.7888, varexp_val = 0.0862, time 1518.17s\n",
      "Early stopping at epoch 30 due to no improvement in validation varexp.\n",
      "Learning rate = 0.0003333333333333333\n",
      "epoch 0, train_loss = 0.7800, val_loss = 0.7862, varexp_val = 0.0895, time 48.97s\n",
      "epoch 1, train_loss = 0.7793, val_loss = 0.7861, varexp_val = 0.0895, time 97.94s\n",
      "epoch 2, train_loss = 0.7791, val_loss = 0.7862, varexp_val = 0.0900, time 146.91s\n",
      "epoch 3, train_loss = 0.7790, val_loss = 0.7861, varexp_val = 0.0898, time 195.89s\n",
      "epoch 4, train_loss = 0.7789, val_loss = 0.7858, varexp_val = 0.0902, time 244.86s\n",
      "epoch 5, train_loss = 0.7787, val_loss = 0.7863, varexp_val = 0.0896, time 293.84s\n",
      "epoch 6, train_loss = 0.7787, val_loss = 0.7861, varexp_val = 0.0902, time 342.81s\n",
      "epoch 7, train_loss = 0.7785, val_loss = 0.7860, varexp_val = 0.0902, time 391.79s\n",
      "epoch 8, train_loss = 0.7785, val_loss = 0.7861, varexp_val = 0.0898, time 440.75s\n",
      "epoch 9, train_loss = 0.7783, val_loss = 0.7860, varexp_val = 0.0904, time 489.73s\n",
      "epoch 10, train_loss = 0.7783, val_loss = 0.7863, varexp_val = 0.0899, time 538.74s\n",
      "epoch 11, train_loss = 0.7782, val_loss = 0.7858, varexp_val = 0.0905, time 587.74s\n",
      "epoch 12, train_loss = 0.7780, val_loss = 0.7857, varexp_val = 0.0907, time 636.70s\n",
      "epoch 13, train_loss = 0.7780, val_loss = 0.7862, varexp_val = 0.0899, time 685.68s\n",
      "epoch 14, train_loss = 0.7778, val_loss = 0.7859, varexp_val = 0.0906, time 734.65s\n",
      "epoch 15, train_loss = 0.7777, val_loss = 0.7860, varexp_val = 0.0904, time 783.62s\n",
      "epoch 16, train_loss = 0.7777, val_loss = 0.7861, varexp_val = 0.0901, time 832.59s\n",
      "epoch 17, train_loss = 0.7776, val_loss = 0.7860, varexp_val = 0.0903, time 881.57s\n",
      "Early stopping at epoch 17 due to no improvement in validation varexp.\n",
      "Learning rate = 0.00011111111111111112\n",
      "epoch 0, train_loss = 0.7763, val_loss = 0.7850, varexp_val = 0.0916, time 48.96s\n",
      "epoch 1, train_loss = 0.7760, val_loss = 0.7850, varexp_val = 0.0916, time 97.93s\n",
      "epoch 2, train_loss = 0.7759, val_loss = 0.7853, varexp_val = 0.0917, time 146.89s\n",
      "epoch 3, train_loss = 0.7759, val_loss = 0.7851, varexp_val = 0.0918, time 195.86s\n",
      "epoch 4, train_loss = 0.7758, val_loss = 0.7850, varexp_val = 0.0918, time 244.85s\n",
      "epoch 5, train_loss = 0.7758, val_loss = 0.7854, varexp_val = 0.0913, time 293.88s\n",
      "epoch 6, train_loss = 0.7757, val_loss = 0.7857, varexp_val = 0.0912, time 342.95s\n",
      "epoch 7, train_loss = 0.7757, val_loss = 0.7852, varexp_val = 0.0918, time 392.02s\n",
      "epoch 8, train_loss = 0.7756, val_loss = 0.7853, varexp_val = 0.0914, time 441.09s\n",
      "epoch 9, train_loss = 0.7756, val_loss = 0.7852, varexp_val = 0.0917, time 490.18s\n",
      "Early stopping at epoch 9 due to no improvement in validation varexp.\n",
      "Learning rate = 3.7037037037037037e-05\n",
      "epoch 0, train_loss = 0.7751, val_loss = 0.7848, varexp_val = 0.0920, time 48.99s\n",
      "epoch 1, train_loss = 0.7750, val_loss = 0.7848, varexp_val = 0.0920, time 98.13s\n",
      "epoch 2, train_loss = 0.7750, val_loss = 0.7852, varexp_val = 0.0920, time 147.22s\n",
      "epoch 3, train_loss = 0.7750, val_loss = 0.7849, varexp_val = 0.0922, time 196.31s\n",
      "epoch 4, train_loss = 0.7749, val_loss = 0.7849, varexp_val = 0.0922, time 245.38s\n",
      "epoch 5, train_loss = 0.7749, val_loss = 0.7852, varexp_val = 0.0918, time 294.39s\n",
      "epoch 6, train_loss = 0.7749, val_loss = 0.7854, varexp_val = 0.0918, time 343.36s\n",
      "epoch 7, train_loss = 0.7749, val_loss = 0.7849, varexp_val = 0.0921, time 392.33s\n",
      "epoch 8, train_loss = 0.7748, val_loss = 0.7849, varexp_val = 0.0920, time 441.30s\n",
      "Early stopping at epoch 8 due to no improvement in validation varexp.\n",
      "saved model ./checkpoints_trained/FX8_051623_3layer_192_192_192_clamp_norm_depthsep_pool.pt\n",
      "loaded model ./checkpoints_trained/FX8_051623_3layer_192_192_192_clamp_norm_depthsep_pool.pt\n",
      "test_pred:  (500, 5804) 0.0119175315 6.559839\n",
      "FEVE (test, all):  0.62631\n",
      "filtering neurons with FEV > 0.15\n",
      "valid neurons: 2217 / 5804\n",
      "FEVE (test, FEV>0.15): 0.7015478610992432\n",
      "core shape:  torch.Size([1, 192, 33, 65])\n",
      "input shape of readout:  (192, 33, 65)\n",
      "model name:  FX8_051623_4layer_192_192_192_192_clamp_norm_depthsep_pool.pt\n",
      "model path:  ./checkpoints_trained/FX8_051623_4layer_192_192_192_192_clamp_norm_depthsep_pool.pt\n",
      "cuda\n",
      "Learning rate = 0.001\n",
      "epoch 0, train_loss = 0.8468, val_loss = 0.8275, varexp_val = 0.0268, time 52.33s\n",
      "epoch 1, train_loss = 0.8196, val_loss = 0.8158, varexp_val = 0.0427, time 104.65s\n",
      "epoch 2, train_loss = 0.8113, val_loss = 0.8102, varexp_val = 0.0515, time 156.97s\n",
      "epoch 3, train_loss = 0.8053, val_loss = 0.8041, varexp_val = 0.0605, time 209.29s\n",
      "epoch 4, train_loss = 0.8007, val_loss = 0.8008, varexp_val = 0.0652, time 261.62s\n",
      "epoch 5, train_loss = 0.7976, val_loss = 0.7986, varexp_val = 0.0689, time 313.98s\n",
      "epoch 6, train_loss = 0.7953, val_loss = 0.7963, varexp_val = 0.0725, time 366.31s\n",
      "epoch 7, train_loss = 0.7935, val_loss = 0.7951, varexp_val = 0.0749, time 418.65s\n",
      "epoch 8, train_loss = 0.7921, val_loss = 0.7940, varexp_val = 0.0765, time 470.98s\n",
      "epoch 9, train_loss = 0.7907, val_loss = 0.7933, varexp_val = 0.0780, time 523.30s\n",
      "epoch 10, train_loss = 0.7896, val_loss = 0.7928, varexp_val = 0.0787, time 575.63s\n",
      "epoch 11, train_loss = 0.7887, val_loss = 0.7921, varexp_val = 0.0797, time 627.95s\n",
      "epoch 12, train_loss = 0.7879, val_loss = 0.7922, varexp_val = 0.0802, time 680.28s\n",
      "epoch 13, train_loss = 0.7873, val_loss = 0.7904, varexp_val = 0.0824, time 732.61s\n",
      "epoch 14, train_loss = 0.7865, val_loss = 0.7914, varexp_val = 0.0814, time 784.93s\n",
      "epoch 15, train_loss = 0.7860, val_loss = 0.7906, varexp_val = 0.0826, time 837.26s\n",
      "epoch 16, train_loss = 0.7855, val_loss = 0.7895, varexp_val = 0.0841, time 889.58s\n",
      "epoch 17, train_loss = 0.7851, val_loss = 0.7898, varexp_val = 0.0839, time 941.90s\n",
      "epoch 18, train_loss = 0.7847, val_loss = 0.7890, varexp_val = 0.0850, time 994.22s\n",
      "epoch 19, train_loss = 0.7844, val_loss = 0.7886, varexp_val = 0.0854, time 1046.54s\n",
      "epoch 20, train_loss = 0.7839, val_loss = 0.7891, varexp_val = 0.0850, time 1098.98s\n",
      "epoch 21, train_loss = 0.7836, val_loss = 0.7888, varexp_val = 0.0855, time 1151.31s\n",
      "epoch 22, train_loss = 0.7833, val_loss = 0.7885, varexp_val = 0.0861, time 1203.63s\n",
      "epoch 23, train_loss = 0.7831, val_loss = 0.7887, varexp_val = 0.0862, time 1255.96s\n",
      "epoch 24, train_loss = 0.7828, val_loss = 0.7885, varexp_val = 0.0855, time 1308.28s\n",
      "epoch 25, train_loss = 0.7826, val_loss = 0.7881, varexp_val = 0.0869, time 1360.60s\n",
      "epoch 26, train_loss = 0.7822, val_loss = 0.7885, varexp_val = 0.0861, time 1412.93s\n",
      "epoch 27, train_loss = 0.7820, val_loss = 0.7876, varexp_val = 0.0876, time 1465.25s\n",
      "epoch 28, train_loss = 0.7818, val_loss = 0.7879, varexp_val = 0.0868, time 1517.57s\n",
      "epoch 29, train_loss = 0.7816, val_loss = 0.7876, varexp_val = 0.0878, time 1569.90s\n",
      "epoch 30, train_loss = 0.7813, val_loss = 0.7888, varexp_val = 0.0864, time 1622.22s\n",
      "epoch 31, train_loss = 0.7812, val_loss = 0.7896, varexp_val = 0.0847, time 1674.54s\n",
      "epoch 32, train_loss = 0.7811, val_loss = 0.7875, varexp_val = 0.0876, time 1726.86s\n",
      "epoch 33, train_loss = 0.7807, val_loss = 0.7875, varexp_val = 0.0879, time 1779.19s\n",
      "epoch 34, train_loss = 0.7807, val_loss = 0.7873, varexp_val = 0.0884, time 1831.52s\n",
      "epoch 35, train_loss = 0.7804, val_loss = 0.7872, varexp_val = 0.0882, time 1883.83s\n",
      "epoch 36, train_loss = 0.7803, val_loss = 0.7869, varexp_val = 0.0889, time 1936.18s\n",
      "epoch 37, train_loss = 0.7802, val_loss = 0.7873, varexp_val = 0.0882, time 1988.59s\n",
      "epoch 38, train_loss = 0.7802, val_loss = 0.7874, varexp_val = 0.0883, time 2040.90s\n",
      "epoch 39, train_loss = 0.7799, val_loss = 0.7879, varexp_val = 0.0876, time 2093.22s\n",
      "epoch 40, train_loss = 0.7799, val_loss = 0.7869, varexp_val = 0.0890, time 2145.56s\n",
      "epoch 41, train_loss = 0.7796, val_loss = 0.7873, varexp_val = 0.0886, time 2197.92s\n",
      "epoch 42, train_loss = 0.7795, val_loss = 0.7879, varexp_val = 0.0876, time 2250.24s\n",
      "epoch 43, train_loss = 0.7794, val_loss = 0.7877, varexp_val = 0.0885, time 2302.55s\n",
      "epoch 44, train_loss = 0.7793, val_loss = 0.7879, varexp_val = 0.0875, time 2354.87s\n",
      "epoch 45, train_loss = 0.7793, val_loss = 0.7868, varexp_val = 0.0890, time 2407.20s\n",
      "epoch 46, train_loss = 0.7790, val_loss = 0.7870, varexp_val = 0.0886, time 2459.52s\n",
      "epoch 47, train_loss = 0.7791, val_loss = 0.7873, varexp_val = 0.0882, time 2511.84s\n",
      "epoch 48, train_loss = 0.7789, val_loss = 0.7879, varexp_val = 0.0876, time 2564.17s\n",
      "epoch 49, train_loss = 0.7788, val_loss = 0.7882, varexp_val = 0.0868, time 2616.50s\n",
      "epoch 50, train_loss = 0.7787, val_loss = 0.7870, varexp_val = 0.0883, time 2668.83s\n",
      "Early stopping at epoch 50 due to no improvement in validation varexp.\n",
      "Learning rate = 0.0003333333333333333\n",
      "epoch 0, train_loss = 0.7751, val_loss = 0.7846, varexp_val = 0.0926, time 52.32s\n",
      "epoch 1, train_loss = 0.7743, val_loss = 0.7845, varexp_val = 0.0926, time 104.64s\n",
      "epoch 2, train_loss = 0.7740, val_loss = 0.7850, varexp_val = 0.0926, time 156.97s\n",
      "epoch 3, train_loss = 0.7739, val_loss = 0.7848, varexp_val = 0.0924, time 209.29s\n",
      "epoch 4, train_loss = 0.7738, val_loss = 0.7849, varexp_val = 0.0923, time 261.61s\n",
      "epoch 5, train_loss = 0.7736, val_loss = 0.7855, varexp_val = 0.0916, time 313.94s\n",
      "Early stopping at epoch 5 due to no improvement in validation varexp.\n",
      "Learning rate = 0.00011111111111111112\n",
      "epoch 0, train_loss = 0.7727, val_loss = 0.7840, varexp_val = 0.0938, time 52.34s\n",
      "epoch 1, train_loss = 0.7722, val_loss = 0.7841, varexp_val = 0.0935, time 104.67s\n",
      "epoch 2, train_loss = 0.7720, val_loss = 0.7845, varexp_val = 0.0936, time 156.99s\n",
      "epoch 3, train_loss = 0.7719, val_loss = 0.7843, varexp_val = 0.0937, time 209.32s\n",
      "epoch 4, train_loss = 0.7718, val_loss = 0.7844, varexp_val = 0.0934, time 261.65s\n",
      "epoch 5, train_loss = 0.7716, val_loss = 0.7850, varexp_val = 0.0926, time 313.98s\n",
      "Early stopping at epoch 5 due to no improvement in validation varexp.\n",
      "Learning rate = 3.7037037037037037e-05\n",
      "epoch 0, train_loss = 0.7716, val_loss = 0.7839, varexp_val = 0.0940, time 52.36s\n",
      "epoch 1, train_loss = 0.7715, val_loss = 0.7840, varexp_val = 0.0939, time 104.72s\n",
      "epoch 2, train_loss = 0.7713, val_loss = 0.7844, varexp_val = 0.0937, time 157.05s\n",
      "epoch 3, train_loss = 0.7713, val_loss = 0.7840, varexp_val = 0.0941, time 209.38s\n",
      "epoch 4, train_loss = 0.7712, val_loss = 0.7842, varexp_val = 0.0939, time 261.70s\n",
      "epoch 5, train_loss = 0.7711, val_loss = 0.7845, varexp_val = 0.0934, time 314.03s\n",
      "epoch 6, train_loss = 0.7710, val_loss = 0.7850, varexp_val = 0.0929, time 366.35s\n",
      "epoch 7, train_loss = 0.7709, val_loss = 0.7845, varexp_val = 0.0934, time 418.67s\n",
      "epoch 8, train_loss = 0.7709, val_loss = 0.7845, varexp_val = 0.0934, time 471.01s\n",
      "Early stopping at epoch 8 due to no improvement in validation varexp.\n",
      "saved model ./checkpoints_trained/FX8_051623_4layer_192_192_192_192_clamp_norm_depthsep_pool.pt\n",
      "loaded model ./checkpoints_trained/FX8_051623_4layer_192_192_192_192_clamp_norm_depthsep_pool.pt\n",
      "test_pred:  (500, 5804) 0.009886086 6.7510643\n",
      "FEVE (test, all):  0.6391685\n",
      "filtering neurons with FEV > 0.15\n",
      "valid neurons: 2217 / 5804\n",
      "FEVE (test, FEV>0.15): 0.7174394130706787\n"
     ]
    }
   ],
   "source": [
    "from minimodel import model_builder\n",
    "from minimodel import model_trainer\n",
    "from minimodel import metrics\n",
    "\n",
    "seed = 1\n",
    "feve_nlayers = []\n",
    "for nlayers in range(1, 5):\n",
    "    # Building Model\n",
    "\n",
    "    nconv1 = 192\n",
    "    nconv2 = 192\n",
    "    model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2)\n",
    "    model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels, seed=seed)\n",
    "    \n",
    "    model_path = os.path.join(weight_path, model_name)\n",
    "    print('model path: ', model_path)\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    # Training the model\n",
    "    print(device)\n",
    "    if not os.path.exists(model_path):\n",
    "        best_state_dict = model_trainer.train(model, spks_train, spks_val, img_train, img_val, device=device)\n",
    "        torch.save(best_state_dict, model_path)\n",
    "        print('saved model', model_path)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print('loaded model', model_path)\n",
    "\n",
    "    # test model\n",
    "    test_pred = model_trainer.test_epoch(model, img_test)\n",
    "    print('test_pred: ', test_pred.shape, test_pred.min(), test_pred.max())\n",
    "\n",
    "\n",
    "    test_fev, test_feve = metrics.feve(spks_rep_all, test_pred)\n",
    "    print('FEVE (test, all): ', np.mean(test_feve))\n",
    "\n",
    "    threshold = 0.15\n",
    "    print(f'filtering neurons with FEV > {threshold}')\n",
    "    valid_idxes = np.where(test_fev > threshold)[0]\n",
    "    print(f'valid neurons: {len(valid_idxes)} / {len(test_fev)}')\n",
    "    print(f'FEVE (test, FEV>0.15): {np.mean(test_feve[test_fev > threshold])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7a005b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering neurons with FEV > 0.15\n",
      "valid neurons: 2217 / 5804\n",
      "FEVE (test): 0.7174394130706787\n"
     ]
    }
   ],
   "source": [
    "from minimodel import metrics\n",
    "test_fev, test_feve = metrics.feve(spks_rep_all, test_pred)\n",
    "\n",
    "threshold = 0.15\n",
    "print(f'filtering neurons with FEV > {threshold}')\n",
    "valid_idxes = np.where(test_fev > threshold)[0]\n",
    "print(f'valid neurons: {len(valid_idxes)} / {len(test_fev)}')\n",
    "print(f'FEVE (test): {np.mean(test_feve[test_fev > threshold])}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimodel-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
