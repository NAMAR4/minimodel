{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from minimodel import data, metrics, model_builder, model_trainer\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "data_path = '../../data'\n",
    "weight_path = '../checkpoints/fullmodel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/../../data/nat60k_text16.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/minimodel-env/lib/python3.10/site-packages/scipy/io/matlab/_mio.py:39\u001b[0m, in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Probably \"not found\"\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/../../data/nat60k_text16.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m mouse_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# load images\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmouse_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_file_name\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmouse_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m nimg, Ly, Lx \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg: \u001b[39m\u001b[38;5;124m'\u001b[39m, img\u001b[38;5;241m.\u001b[39mshape, img\u001b[38;5;241m.\u001b[39mmin(), img\u001b[38;5;241m.\u001b[39mmax(), img\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m/mnt/vast-nhr/projects/bthesis_cidas_richter/benjamin/minimodel/minimodel/data.py:44\u001b[0m, in \u001b[0;36mload_images\u001b[0;34m(root, mouse_id, file, downsample, normalize, crop)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" \u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03mload images from mat file.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    img: The preprocessed images with shape (n_images, Ly, Lx).\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file)\n\u001b[0;32m---> 44\u001b[0m dstim \u001b[38;5;241m=\u001b[39m \u001b[43mloadmat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msqueeze_me\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# stimulus data\u001b[39;00m\n\u001b[1;32m     45\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(dstim[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m'\u001b[39m], (\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m dstim\n",
      "File \u001b[0;32m~/.conda/envs/minimodel-env/lib/python3.10/site-packages/scipy/io/matlab/_mio.py:233\u001b[0m, in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, spmatrix, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03mLoad MATLAB file.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m    3.14159265+3.14159265j])\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    232\u001b[0m variable_names \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariable_names\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 233\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_context(file_name, appendmat) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    234\u001b[0m     MR, _ \u001b[38;5;241m=\u001b[39m mat_reader_factory(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    235\u001b[0m     matfile_dict \u001b[38;5;241m=\u001b[39m MR\u001b[38;5;241m.\u001b[39mget_variables(variable_names)\n",
      "File \u001b[0;32m~/.conda/envs/minimodel-env/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/minimodel-env/lib/python3.10/site-packages/scipy/io/matlab/_mio.py:17\u001b[0m, in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_context\u001b[39m(file_like, appendmat, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     f, opened \u001b[38;5;241m=\u001b[39m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappendmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "File \u001b[0;32m~/.conda/envs/minimodel-env/lib/python3.10/site-packages/scipy/io/matlab/_mio.py:45\u001b[0m, in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m appendmat \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_like\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     44\u001b[0m         file_like \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReader needs file name or open file-like object\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     49\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/../../data/nat60k_text16.mat'"
     ]
    }
   ],
   "source": [
    "mouse_id = 0\n",
    "\n",
    "# load images\n",
    "img = data.load_images(data_path, mouse_id, file=os.path.join(data_path, data.img_file_name[mouse_id]))\n",
    "nimg, Ly, Lx = img.shape\n",
    "print('img: ', img.shape, img.min(), img.max(), img.dtype)\n",
    "\n",
    "# load neurons\n",
    "fname = '%s_nat60k_%s.npz'%(data.db[mouse_id]['mname'], data.db[mouse_id]['datexp'])\n",
    "spks, istim_train, istim_test, xpos, ypos, spks_rep_all = data.load_neurons(file_path = os.path.join(data_path, fname), mouse_id = mouse_id)\n",
    "n_stim, n_max_neurons = spks.shape\n",
    "\n",
    "# split train and validation set\n",
    "itrain, ival = data.split_train_val(istim_train, train_frac=0.9)\n",
    "\n",
    "# normalize spks\n",
    "spks, spks_rep_all = data.normalize_spks(spks, spks_rep_all, itrain)\n",
    "\n",
    "ineur = np.arange(0, n_max_neurons) #np.arange(0, n_neurons, 5)\n",
    "\n",
    "spks_val = torch.from_numpy(spks[ival][:,ineur]) \n",
    "spks_rep_all = [spks_rep_all[i][:,ineur] for i in range(len(spks_rep_all))]\n",
    "\n",
    "img_val = torch.from_numpy(img[istim_train][ival]).to(device).unsqueeze(1)\n",
    "img_test = torch.from_numpy(img[istim_test]).to(device).unsqueeze(1)\n",
    "\n",
    "input_Ly, input_Lx = img_test.shape[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntest_trails = 0\n",
    "for i in range(len(spks_rep_all)):\n",
    "    ntest_trails += spks_rep_all[i].shape[0]\n",
    "print('ntest_trails: ', ntest_trails)\n",
    "print('total_trails: ', spks.shape[0]+ntest_trails)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n layers result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "nlayers = 2\n",
    "feve_nlayers = []\n",
    "for nlayers in range(1, 5):\n",
    "    nconv1 = 192\n",
    "    nconv2 = 192\n",
    "    model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2)\n",
    "    model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels, seed=seed)\n",
    "\n",
    "    weight_path = os.path.join(weight_path, 'fullmodel', data.mouse_names[mouse_id])\n",
    "    if not os.path.exists(weight_path):\n",
    "        os.makedirs(weight_path)\n",
    "    model_path = os.path.join(weight_path, model_name)\n",
    "    print('model path: ', model_path)\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print('loaded model', model_path)\n",
    "\n",
    "    # test model\n",
    "    test_pred = model_trainer.test_epoch(model, img_test)\n",
    "    test_fev, test_feve = metrics.feve(spks_rep_all, test_pred)\n",
    "    print('FEVE (test, all): ', np.mean(test_feve))\n",
    "\n",
    "    threshold = 0.15\n",
    "    print(f'filtering neurons with FEV > {threshold}')\n",
    "    valid_idxes = np.where(test_fev > threshold)[0]\n",
    "    print(f'valid neurons: {len(valid_idxes)} / {len(test_fev)}')\n",
    "    print(f'FEVE (test, FEV>0.15): {np.mean(test_feve[test_fev > threshold])}')\n",
    "\n",
    "    feve_nlayers.append(test_feve)\n",
    "\n",
    "feve_nlayers = np.stack(feve_nlayers)\n",
    "print(feve_nlayers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['feve_depth'] = feve_nlayers\n",
    "data_dict['valid_idxes'] = valid_idxes\n",
    "data_dict['fev'] = test_fev"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change #conv1 #conv2 result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "nlayers = 2\n",
    "nconv1 = 192\n",
    "nconv2 = 192\n",
    "nconv1_list = [8,16,32,64, 128, 192, 256, 320, 384, 448]\n",
    "nconv2_list = [8,16,32,64, 128, 192, 256, 320, 384, 448]\n",
    "seed = 1\n",
    "n_valid_neurons = len(valid_idxes)\n",
    "feve_width = np.zeros((len(nconv1_list), len(nconv2_list), n_valid_neurons))\n",
    "data\n",
    "for i, nconv1 in enumerate(nconv1_list):\n",
    "    for j, nconv2 in enumerate(nconv2_list):\n",
    "        model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2)\n",
    "        model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels, seed=seed)\n",
    "\n",
    "        weight_path = os.path.join(weight_path, 'fullmodel', data.mouse_names[mouse_id])\n",
    "        if not os.path.exists(weight_path):\n",
    "            os.makedirs(weight_path)\n",
    "        model_path = os.path.join(weight_path, model_name)\n",
    "        print('model path: ', model_path)\n",
    "        model = model.to(device)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print('loaded model', model_path)\n",
    "\n",
    "        # test model\n",
    "        test_pred = model_trainer.test_epoch(model, img_test)\n",
    "        test_fev, test_feve = metrics.feve(spks_rep_all, test_pred)\n",
    "        print('FEVE (test, all): ', np.mean(test_feve))\n",
    "\n",
    "        threshold = 0.15\n",
    "        print(f'filtering neurons with FEV > {threshold}')\n",
    "        valid_idxes = np.where(test_fev > threshold)[0]\n",
    "        print(f'valid neurons: {len(valid_idxes)} / {len(test_fev)}')\n",
    "        print(f'FEVE (test, FEV>0.15): {np.mean(test_feve[test_fev > threshold])}')\n",
    "        feve_width[i,j] = np.mean(test_feve[test_fev > threshold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['feve_width'] = feve_width"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change #stims train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_numbers = np.geomspace(500, 30000, num=10, dtype=int)\n",
    "stim_numbers = np.unique(stim_numbers)  # Remove duplicates that might occur due to rounding\n",
    "print(stim_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "seed = 1\n",
    "nlayers = 2\n",
    "nconv1 = 16\n",
    "nconv2 = 320\n",
    "n_max_neurons = len(valid_idxes)\n",
    "feve_nstims = np.zeros((len(stim_numbers), n_max_neurons))\n",
    "\n",
    "for i, n_stim in enumerate(stim_numbers):\n",
    "    if n_stim  > len(itrain): n_stim = len(itrain)\n",
    "    \n",
    "    suffix = f'nstims_{n_stim}'\n",
    "    model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2)\n",
    "    model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels, seed=seed, suffix=suffix)\n",
    "\n",
    "    weight_path = os.path.join(weight_path, 'fullmodel', data.mouse_names[mouse_id])\n",
    "    if not os.path.exists(weight_path):\n",
    "        os.makedirs(weight_path)\n",
    "    model_path = os.path.join(weight_path, model_name)\n",
    "    print('model path: ', model_path)\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print('loaded model', model_path)\n",
    "\n",
    "    # test model\n",
    "    test_pred = model_trainer.test_epoch(model, img_test)\n",
    "    test_fev, test_feve = metrics.feve(spks_rep_all, test_pred)\n",
    "    print('FEVE (test): ', np.mean(test_feve[valid_idxes]))\n",
    "\n",
    "    feve_nstims[i] = np.mean(test_feve[valid_idxes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['feve_nstims'] = feve_nstims\n",
    "data_dict['nstims'] = stim_numbers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change #neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_numbers = np.geomspace(1, 1000, num=10, dtype=int)\n",
    "neuron_numbers = np.unique(np.concatenate(([1], neuron_numbers)))  # Ensure 1 is included and remove duplicates\n",
    "seed_numbers = np.linspace(10, 1, num=len(neuron_numbers), dtype=int)\n",
    "print(neuron_numbers)\n",
    "print(seed_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "seed = 1\n",
    "nlayers = 2\n",
    "nconv1 = 16\n",
    "nconv2 = 320\n",
    "\n",
    "feve_nneurons = []\n",
    "\n",
    "for i, n_neuron in enumerate(neuron_numbers):\n",
    "    feve_nneurons.append([])\n",
    "    for seed in range(1, seed_numbers[i]+1):\n",
    "        np.random.seed(n_neuron*seed)\n",
    "        if n_neuron < len(valid_idxes):\n",
    "            ineur = np.random.choice(valid_idxes, size=n_neuron, replace=False)\n",
    "        else:\n",
    "            ineur = valid_idxes\n",
    "            n_neuron = len(valid_idxes)\n",
    "            \n",
    "        suffix = f'nneurons_{n_neuron}'\n",
    "        spks_train = torch.from_numpy(spks[itrain][:,ineur])\n",
    "        spks_val = torch.from_numpy(spks[ival][:,ineur]) \n",
    "        spks_rep = [spks_rep_all[i][:,ineur] for i in range(len(spks_rep_all))]\n",
    "\n",
    "        \n",
    "        model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2)\n",
    "        model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels, seed=seed, suffix=suffix)\n",
    "\n",
    "        weight_path = os.path.join(weight_path, 'fullmodel', data.mouse_names[mouse_id])\n",
    "        if not os.path.exists(weight_path):\n",
    "            os.makedirs(weight_path)\n",
    "        model_path = os.path.join(weight_path, model_name)\n",
    "        print('model path: ', model_path)\n",
    "        model = model.to(device)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print('loaded model', model_path)\n",
    "\n",
    "        # test model\n",
    "        test_pred = model_trainer.test_epoch(model, img_test)\n",
    "        test_fev, test_feve = metrics.feve(spks_rep, test_pred)\n",
    "        print('FEVE (test): ', np.mean(test_feve))\n",
    "\n",
    "        feve_nneurons[i].append(np.mean(test_feve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feve_nneurons = [np.mean(x) for x in feve_nneurons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['feve_nneurons'] = feve_nneurons\n",
    "data_dict['nneurons'] = neuron_numbers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlayers = 2\n",
    "nconv1 = 16\n",
    "nconv2 = 320\n",
    "n_stim, n_max_neurons = spks.shape\n",
    "ineur = np.arange(0, n_max_neurons) #np.arange(0, n_neurons, 5)\n",
    "model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2)\n",
    "model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels, seed=seed, suffix=suffix)\n",
    "\n",
    "weight_path = os.path.join(weight_path, 'fullmodel', data.mouse_names[mouse_id])\n",
    "if not os.path.exists(weight_path):\n",
    "    os.makedirs(weight_path)\n",
    "model_path = os.path.join(weight_path, model_name)\n",
    "print('model path: ', model_path)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "print('loaded model', model_path)\n",
    "\n",
    "# test model\n",
    "test_pred = model_trainer.test_epoch(model, img_test)\n",
    "test_fev, test_feve = metrics.feve(spks_rep_all, test_pred)\n",
    "print('FEVE (test, all): ', np.mean(test_feve))\n",
    "\n",
    "threshold = 0.15\n",
    "print(f'filtering neurons with FEV > {threshold}')\n",
    "valid_idxes = np.where(test_fev > threshold)[0]\n",
    "print(f'valid neurons: {len(valid_idxes)} / {len(test_fev)}')\n",
    "print(f'FEVE (test, FEV>0.15): {np.mean(test_feve[test_fev > threshold])}')\n",
    "\n",
    "data_dict['fullmodel_Wx'] = model.readout.Wx.cpu().detach().numpy().squeeze()\n",
    "data_dict['fullmodel_Wy'] = model.readout.Wy.cpu().detach().numpy().squeeze()\n",
    "data_dict['fullmodel_feve_all'] = test_feve"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LN model result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlayers = 2\n",
    "nconv1 = 16\n",
    "nconv2 = 320\n",
    "seed = 1\n",
    "suffix = ''\n",
    "suffix += 'LN'\n",
    "n_stim, n_max_neurons = spks.shape\n",
    "ineur = np.arange(0, n_max_neurons) \n",
    "model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2, activation=None, avgpool=True)\n",
    "model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels, seed=seed, suffix=suffix)\n",
    "\n",
    "weight_path = os.path.join(weight_path, 'LNmodel', data.mouse_names[mouse_id])\n",
    "if not os.path.exists(weight_path):\n",
    "    os.makedirs(weight_path)\n",
    "model_path = os.path.join(weight_path, model_name)\n",
    "print('model path: ', model_path)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "print('loaded model', model_path)\n",
    "\n",
    "# test model\n",
    "test_pred = model_trainer.test_epoch(model, img_test)\n",
    "test_fev, test_feve = metrics.feve(spks_rep_all, test_pred)\n",
    "print('FEVE (test, all): ', np.mean(test_feve))\n",
    "\n",
    "threshold = 0.15\n",
    "print(f'filtering neurons with FEV > {threshold}')\n",
    "valid_idxes = np.where(test_fev > threshold)[0]\n",
    "print(f'valid neurons: {len(valid_idxes)} / {len(test_fev)}')\n",
    "print(f'FEVE (test, FEV>0.15): {np.mean(test_feve[test_fev > threshold])}')\n",
    "\n",
    "data_dict['LNmodel_feve_all'] = test_feve\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(f'outputs/fullmodel_{data.mouse_names[mouse_id]}_results.npz', **data_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimodel-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
